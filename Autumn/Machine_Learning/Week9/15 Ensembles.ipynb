{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles\n",
    "1. Bagging  \n",
    "2. Random Subspace \n",
    "3. Boosting\n",
    "4. Feature Importance from Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hotel_rev_pd = pd.read_csv('HotelRevHelpfulnessV2.csv')\n",
    "hotel_rev_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hotel_rev_pd.pop('reviewHelpfulness').values\n",
    "X = hotel_rev_pd.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "kNN = KNeighborsClassifier(n_neighbors=3) \n",
    "dtree = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "Ensembles based on Bagging. \n",
    "- 10 ensemble members are trained using bootstrap resampling\n",
    "- Works for decision trees\n",
    "- Doesn't work for k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNN_bag = BaggingClassifier(kNN, \n",
    "                            n_estimators = 10,\n",
    "                            max_samples = 1.0, \n",
    "                            bootstrap = True)\n",
    "\n",
    "tree_bag = BaggingClassifier(dtree, \n",
    "                            n_estimators = 10,\n",
    "                            max_samples = 1.0, # bootstrap resampling \n",
    "                            bootstrap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 8\n",
    "reps = 10\n",
    "v = 10\n",
    "cv=RepeatedKFold(n_repeats=reps, n_splits=folds)\n",
    "\n",
    "scores_kNN = cross_val_score(kNN, X, y, cv=cv, verbose = v, n_jobs = -1)\n",
    "scores_kNN_bag = cross_val_score(kNN_bag, X, y, cv=cv, verbose = v, n_jobs = -1)\n",
    "\n",
    "print(\"Mean for kNN {:.2f}\".format(scores_kNN.mean()))\n",
    "print(\"Mean for kNN_bag {:.2f}\".format(scores_kNN_bag.mean()))\n",
    "\n",
    "scores_tree = cross_val_score(dtree, X, y, cv=cv, verbose = v, n_jobs = -1)\n",
    "scores_tree_bag = cross_val_score(tree_bag, X, y, cv=cv, verbose = v, n_jobs = -1)\n",
    "\n",
    "print(\"Mean for D-Tree {:.2f}\".format(scores_tree.mean()))\n",
    "print(\"Mean for D_Tree_bag {:.2f}\".format(scores_tree_bag.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Random Subspace\n",
    "The evaluation above shows that bootstrap resampling works for decision tree ensembles but not for k-NN.   \n",
    "This is because k-NN is a *stable* classifier so boodstrap resampling does not produce diversity.  \n",
    "  \n",
    "However a random subspace strategy will produce diversity for k-NN.  \n",
    "In the examples below we generate an ensemble of 10 classifiers each trained using a subet of 50% of the features selected at random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_SS_kNN = BaggingClassifier(kNN, \n",
    "                            n_estimators = 10,\n",
    "                            max_samples=1.0, \n",
    "                            max_features=0.5)\n",
    "\n",
    "random_SS_tree = BaggingClassifier(dtree, \n",
    "                            n_estimators = 10,\n",
    "                            max_samples=1.0, \n",
    "                            max_features=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 8\n",
    "reps = 10\n",
    "v = 0\n",
    "cv=RepeatedKFold(n_repeats=reps, n_splits=folds)\n",
    "\n",
    "scores_kNN = cross_val_score(kNN, X, y, cv=cv, verbose = v, n_jobs = -1)\n",
    "scores_kNN_rSS = cross_val_score(random_SS_kNN, X, y, cv=cv, verbose = v, n_jobs = -1)\n",
    "\n",
    "print(\"Mean for kNN {:.2f}\".format(scores_kNN.mean()))\n",
    "print(\"Mean for kNN_rand_SS {:.2f}\".format(scores_kNN_rSS.mean()))\n",
    "\n",
    "scores_tree = cross_val_score(dtree, X, y, cv=cv, verbose = v, n_jobs = -1)\n",
    "scores_tree_rSS = cross_val_score(random_SS_tree, X, y, cv=cv, verbose = v, n_jobs = -1)\n",
    "\n",
    "print(\"Mean for D-Tree {:.2f}\".format(scores_tree.mean()))\n",
    "print(\"Mean for D_Tree_rand_SS {:.2f}\".format(scores_tree_rSS.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Boosting\n",
    "Default classifier is a Decision Tree of depth 1, a decision stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaBoost = AdaBoostClassifier(n_estimators=100, algorithm = 'SAMME')\n",
    "scores_adaBoost = cross_val_score(adaBoost, X, y, cv=folds, verbose = v, n_jobs = -1)\n",
    "\n",
    "scores_adaBoost.mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Adaboost on all data and check weights for first 10 models\n",
    "ab = adaBoost.fit(X,y)\n",
    "ab.estimator_weights_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ab.estimator_errors_, label='Errors')\n",
    "plt.plot(ab.estimator_weights_, label='Weights')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error & Weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1><span style=\"color:red\">Bonus Material</span></h1>\n",
    "\n",
    "## Random Forest Feature Importance\n",
    "As a side effect of building so many decision trees Random Forest is able to provide an estimate of feature importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "RF.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FI_df = pd.DataFrame(RF.feature_importances_, index=hotel_rev_pd.columns,columns =['FI Score'])\n",
    "FI_df.sort_values('FI Score', inplace=True, ascending = False)\n",
    "FI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pl = FI_df.plot.bar(figsize=(10,5))\n",
    "pl.set_ylabel(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
