{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76653f30",
   "metadata": {},
   "source": [
    "# Selecting Features for Classification\n",
    "\n",
    "In this notebook we will look at how using different feature subsets from a dataset can impact on classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41073502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 13})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf79639",
   "metadata": {},
   "source": [
    "For this example, we will use the [*Diabetes* dataset](https://www.kaggle.com/datasets/mathchi/diabetes-data-set), originally collected by the US National Institute of Diabetes. Given a set of patient diagnostic measurements, the objective of this task is to predict whether or not the patient has diabetes. The features in the data are as follows:\n",
    "\n",
    "- *preg*: Number of times the patient has been pregnant\n",
    "- *plasma*: Plasma glucose concentration\n",
    "- *pres*: Diastolic blood pressure\n",
    "- *skin*: Triceps skin fold thickness\n",
    "- *insulin*: 2-Hour serum insulin level\n",
    "- *bmi*: Body mass index \n",
    "- *family*: Score likelihood of diabetes based on family history\n",
    "- *age*: The patient's age in years\n",
    "- *outcome*: The actual diagnosis for the patient ('positive' or 'negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5a36e",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79452134",
   "metadata": {},
   "source": [
    "Load the complete dataset from a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2495ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2232c489",
   "metadata": {},
   "source": [
    "Separate out the features to use for classification from the target label itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[\"outcome\"].values\n",
    "features = list(df.columns)\n",
    "features.remove(\"outcome\")\n",
    "print(\"Features: %s\" % features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee023d6",
   "metadata": {},
   "source": [
    "Firstly, try binary classification with the full feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6820c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[features]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b2bb0c",
   "metadata": {},
   "source": [
    "We will use 5-fold cross validation to evaluate a KNN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "acc_scores = cross_val_score(knn, data, target, cv=2, scoring=\"accuracy\")\n",
    "mean_accuracy = np.mean(acc_scores)\n",
    "print(\"Mean cross validation accuracy = %.3f\" % mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e89eca",
   "metadata": {},
   "source": [
    "We could decide to manually choose a subset of features that we might believe are particularly relevant for the problem. For instance:\n",
    "\n",
    "- *insulin*: 2-Hour serum insulin level\n",
    "- *bmi*: Body mass index \n",
    "- *family*: Score likelihood of diabetes based on family history\n",
    "- *age*: The patient's age in years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d62c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1 = [\"insulin\", \"bmi\", \"family\", \"age\"]\n",
    "data_subset1 = data[subset1]\n",
    "data_subset1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd19a9",
   "metadata": {},
   "source": [
    "We will apply the same classifier and evaluation process on the subset of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4f8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "acc_scores = cross_val_score(knn, data_subset1, target, cv=2, scoring=\"accuracy\")\n",
    "mean_accuracy = np.mean(acc_scores)\n",
    "print(\"Mean cross validation accuracy = %.3f\" % mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125ce32",
   "metadata": {},
   "source": [
    "If this does not work, we might manually choose an alternative subset of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2 = [\"preg\", \"plasma\", \"bmi\", \"family\", \"age\"]\n",
    "data_subset2 = data[subset2]\n",
    "data_subset2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5321b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "acc_scores = cross_val_score(knn, data_subset2, target, cv=2, scoring=\"accuracy\")\n",
    "mean_accuracy = np.mean(acc_scores)\n",
    "print(\"Mean cross validation accuracy = %.3f\" % mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edacca86",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Rather than manually selecting features, we could automate this process to search for a set of features that leads to the maximum classification accuracy.\n",
    "\n",
    "For more information see:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94370fc3",
   "metadata": {},
   "source": [
    "One way to conduct the search process is *forward selection*. Here we start with an empty set of features. At each step, the best of the original features is determined and added to the current set. The actual determination of the best feature at each step is based on the cross-validation score achieved using that feature when combined with a specified classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier to use during the search\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# perform the search up to the specified number of features\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    knn, n_features_to_select=3, direction=\"forward\"\n",
    ").fit(data, target)\n",
    "\n",
    "# get the boolean mask of selected features\n",
    "feature_mask = sfs_forward.get_support()\n",
    "data_forward = data.iloc[:, feature_mask]\n",
    "# get the names of the selected featured\n",
    "features_forward = list(data_forward.columns)\n",
    "print(\"Selected features:\", features_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c41725",
   "metadata": {},
   "source": [
    "We can use these features then for a subsequent classification process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8930591",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = cross_val_score(knn, data_forward, target, cv=2, scoring=\"accuracy\")\n",
    "mean_accuracy = np.mean(acc_scores)\n",
    "print(\"Mean cross validation accuracy = %.3f\" % mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e8376",
   "metadata": {},
   "source": [
    "Another approach for searching for a good feature subset is *backward selection* (also called *backward elimination*). This strategy begins with the full set of features. At each step, it removes the worst feature remaining in the current set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29121049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier to use during the search\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# perform the search down to the specified number of features\n",
    "sfs_backward = SequentialFeatureSelector(\n",
    "    knn, n_features_to_select=3, direction=\"backward\"\n",
    ").fit(data, target)\n",
    "\n",
    "# get the boolean mask of selected features\n",
    "feature_mask = sfs_backward.get_support()\n",
    "data_backward = data.iloc[:, feature_mask]\n",
    "# get the names of the selected featured\n",
    "features_backward = list(data_backward.columns)\n",
    "print(\"Selected features:\", features_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c648e71",
   "metadata": {},
   "source": [
    "Again we can use these features then for a subsequent classification process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b86c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = cross_val_score(knn, data_backward, target, cv=2, scoring=\"accuracy\")\n",
    "mean_accuracy = np.mean(acc_scores)\n",
    "print(\"Mean cross validation accuracy = %.3f\" % mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02846f43",
   "metadata": {},
   "source": [
    "Note that both of the search procedures are \"greedy\" (i.e. they make locally optimal choice at each stage). This speeds up the feature selection process, but means that we are not guarateed to find the globally optimal result. Also, this means that *forward selection* and *backward selection* can generated different feature subsets for the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cf3bc9",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Rather than selecting subsets of features, an alternative approach is produce some kind of ranking of the features, from \"best\" to \"worst\".\n",
    "\n",
    "One ranking approach is to calculate the *permutation importance* of a feature is calculated as follows. First, a baseline score is calculated on the original dataset. Next, a feature is \"permuted\" (e.g. the values are randomly shuffled) and the score is calculated again. The permutation importance is defined to be the difference between the baseline score and the score after permutating the feature. This process is repeated multiple times for each feature, so that we can calculate mean importances scores for every feature. Better feature should have higher importance scores.\n",
    "\n",
    "\n",
    "For more information see:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1581621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(data, target)\n",
    "result = permutation_importance(knn, data, target, n_repeats=10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca428f52",
   "metadata": {},
   "source": [
    "We could the display the mean permutation importance scores for each feature in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42bf888",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = result.importances_mean.argsort()\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(data.columns[sorted_idx], result.importances[sorted_idx].mean(axis=1).T, color=\"darkorange\", zorder=3)\n",
    "ax.xaxis.grid(True)\n",
    "ax.set_xlabel(\"Permuation Importance Score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955132f3",
   "metadata": {},
   "source": [
    "We could decide to use some of the top-ranked features from above in a subsequent classification process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55768f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we need to reverse the order\n",
    "ranked_features = list(data.columns[sorted_idx])\n",
    "ranked_features.reverse()\n",
    "# we could pick the top 4 in this case\n",
    "subset_top4 = ranked_features[0:4]\n",
    "subset_top4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_top4 = data[subset_top4]\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "acc_scores = cross_val_score(knn, data_top4, target, cv=2, scoring=\"accuracy\")\n",
    "mean_accuracy = np.mean(acc_scores)\n",
    "print(\"Mean cross validation accuracy = %.3f\" % mean_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
