{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification involves using labeled (known) training examples to predict a class label for unseen input examples. In this lab we will use the classification functionality provided by the *scitkit-learn* Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "For the examples in this notebook, we will use the *Penguin* dataset described [here](https://www.kaggle.com/parulpandey/penguin-dataset-the-new-iris).\n",
    "\n",
    "The dataset contains records related to penguins collected from islands in the Palmer Archipelago, Antarctica. Each penguin belongs to one of 3 species (Adelie', 'Gentoo', 'Chinstrap'). We would like to train a classification algorithm to automatically classify a record describing a penguin into one of the three species (classes).\n",
    "\n",
    "The features in the data are as follows:\n",
    "\n",
    "- *island*: The name of the island in Antarctica where the penguin was found ('Dream', 'Torgersen', or 'Biscoe') \n",
    "- *bill_length*: Length of the penguin's bill in mm\n",
    "- *bill_depth*: Depth of the penguin's bill in mm\n",
    "- *flipper_length*: Length of the penguin's flipper in mm\n",
    "- *body_mass*: Weight of the penguin in grams\n",
    "- *sex*: Indicates whether the penguin is male or female\n",
    "- *species*: The species of this penguin (either 'Adelie', 'Gentoo', or 'Chinstrap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('penguins.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example in the dataset has a class label or a \"target\" from three possible classes. We can look at the distribution of these classes (i.e. the number of penguins in each species):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"species\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of classification, we will focus on the numeric features in the data and remove the non-numeric features ('island' and 'sex'). We will also separate out the feature values from the target label ('species')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[\"species\"].values\n",
    "data = df[[\"bill_length\", \"bill_depth\", \"flipper_length\", \"body_mass\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the various numeric features in the data all have different ranges, we will apply min-max normalisation to transfrom them in to the range [0,1]. We can use the *MinMaxScaler* in scikit-learn to do this. Note the output will be a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler()  \n",
    "data_scaled = normalizer.fit_transform(data.values)\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Classification\n",
    "\n",
    "As our classification algorithm in this notebook, we will focus on the use of the simple but effectives *k-Nearest Neighbour (KNN) classifier*. Given a new input example, this algorithm finds the most similar previous examples for which a decision has already been made (i.e. their nearest neighbours from the training set). Based on the majority vote among the K neighbours, a prediction is made for the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a nearest neighbour classifier using just 1 nearest neighbour. \n",
    "In this case we will use the full dataset and all of the target labels for those examples.  \n",
    "Because the ranges of the feature values are quite different we scale them all to the range [0,1] so they all have the same impact on classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "m = knn.fit(data_scaled, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test it out by making a prediction for a new input example described by 4 feature values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our new penguin record\n",
    "penguin1 = [39.1, 16.8, 180.5, 3705.0]\n",
    "unseen_data = np.array([penguin1])\n",
    "# apply the same min-max- normalisation as before\n",
    "unseen_scaled = normalizer.transform(unseen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the prediction, the output is the class label\n",
    "prediction = knn.predict(unseen_scaled)\n",
    "prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make predictions for multiple unseen input examples at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin1 = [39.1, 16.8, 180.5, 3705]\n",
    "penguin2 = [46.2, 14.9, 208, 5286]\n",
    "penguin3 = [50.3, 18.8, 201.5, 3804]\n",
    "penguin4 = [40.1, 17.3, 185, 3402]\n",
    "unseen_data = np.array([penguin1, penguin2, penguin3, penguin4])\n",
    "# normalize the input data\n",
    "unseen_scaled = normalizer.transform(unseen_data)\n",
    "# make the predictions for the 4 unseen examples\n",
    "knn.predict(unseen_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training and Test Sets\n",
    "\n",
    "A key task when applying a classifier is to determine how effective our classifier will be at making predictions. One way to estimate this is to divide the full dataset into two sets using a \"hold-out strategy\":\n",
    "1. *Training set*: A set of examples used to build the classification model.\n",
    "2. *Test set*: A separate set of examples that is withheld from the classifier during training, and is used afterwards to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the effectiveness of our KNN classifier on the penguin data, we will randomly split the complete dataset into a training test (used to build the model) and an unseen test set (used to try out and evaluate the model). Scikit-learn provides functionality to do this. We will specify that 30% (0.3) of the data will be used for the test set. By specifiying a value for the *random_state*, we can reproduce the same results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# use 70% for training, 30% for testing\n",
    "data_train, data_test, target_train, target_test = train_test_split(data_scaled, target, \n",
    "    test_size=0.3, random_state=1)\n",
    "print(\"Training set has %d examples\" % data_train.shape[0])\n",
    "print(\"Test set has %d examples\" % data_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have performed our split, we then train our model based only on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "m = model.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions for the test set, based on the model that we just trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(data_test)\n",
    "# print the predictions\n",
    "print(\"Predictions:\\n%s\" % predicted)\n",
    "# print the number of predictions from each class\n",
    "print(\"Class counts:\\n%s\" % pd.Series(predicted).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our predictions to the \"correct answer\" based on the labels for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions\\n\", predicted)\n",
    "print(\"Correct labels\\n\", target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quantitatively check how accurate these predictions are, by measuring *accuracy*, which will return a value between 0.0 (predictions are completely wrong) and 1.0 (predictions are 100% accurate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(target_test, predicted)\n",
    "print(\"Accuracy=%.3f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Parameters\n",
    "\n",
    "Many classification algorithms have one or more parameter values which control various aspects of the learning process. In the case of a KNN classifier, the key parameter is the number of neighbours *k* considered when making a prediction. Different values for this parameter can lead to different predictions on unseen data, resulting in higher levels of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the training/test split that we created above, we will examine the effect of increasing the parameter for the number of neighbours *k* on the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over a range of values of k\n",
    "for k in range(1, 16):\n",
    "    # train a classifier with this parameter value\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    m = model.fit(data_train, target_train)\n",
    "    # make predictions\n",
    "    predicted = model.predict(data_test)\n",
    "    # evaluate the predictions\n",
    "    acc = accuracy_score(target_test, predicted)\n",
    "    print(\"K=%02d neighbours: Accuracy=%.3f\" % (k, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a little variation in the accuracy results above, although for this dataset the algorithm is not very sensitive to the choice of parameter value for KNN. However, this may not be the case for more difficult classification tasks and more complex datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
