{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Classification\n",
    "\n",
    "In this notebook we will look at further aspects of classification, including performing a robust evaluation of a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# imports for plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use the [*Diabetes* dataset](https://www.kaggle.com/datasets/mathchi/diabetes-data-set), originally collected by the US National Institute of Diabetes. Given a set of patient diagnostic measurements, the objective of this task is to predict whether or not the patient has diabetes. The features in the data are as follows:\n",
    "\n",
    "- *preg*: Number of times the patient has been pregnant\n",
    "- *plasma*: Plasma glucose concentration\n",
    "- *pres*: Diastolic blood pressure\n",
    "- *skin*: Triceps skin fold thickness\n",
    "- *insulin*: 2-Hour serum insulin level\n",
    "- *bmi*: Body mass index \n",
    "- *family*: Score likelihood of diabetes based on family history\n",
    "- *age*: The patient's age in years\n",
    "- *outcome*: The actual diagnosis for the patient ('positive' or 'negative')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the complete dataset from a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the distribution of the target label 'outcome' in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out the features to use for classification from the target label itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[\"outcome\"].values\n",
    "other_columns = list(df.columns)\n",
    "other_columns.remove(\"outcome\")\n",
    "data = df[other_columns]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will apply normalisation to the numeric data. In this case, the variables are mean centered and scaled by the standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = StandardScaler()\n",
    "data_scaled = normalizer.fit_transform(data.values)\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We will now look at different ways to evaluate a classification algorithm. We will focus on the case of *binary classification* (two classes) using the diabetes data, although these methods generalise to *multi-class* problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen previously, we can easily randomly split the complete dataset into a training test and a test set. We will specify that 40% (0.4) of the data will be used for the test set. The remaining 60% will be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(data_scaled, target, test_size=0.4)\n",
    "print(\"Training set has %d examples\" % data_train.shape[0])\n",
    "print(\"Test set has %d examples\" % data_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a KNN classifier (*k=3* neighbours) based on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "m = knn.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make our predictions on the test set and evaluate them using the standard classification *accuracy* score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = knn.predict(data_test)\n",
    "# calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(target_test, predicted)\n",
    "print(\"Accuracy=%.3f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy scores only provide us with a single score summarising a classifier's performance. Often we want to understand where a classifier performs well or poorly in more detail.\n",
    "\n",
    "In particular, we might want to determine the extent to which the classifier made the following correct/incorrect predictions:\n",
    "- *True Positives* (TP) are those which are labeled 'positive' which are actually 'positive'\n",
    "- *False Positives* (FP) are those which are labeled 'positive' which are actually 'negative'\n",
    "- *True Negatives* (TN) are those which are labeled 'negative' which are actually 'negative'\n",
    "- *False Negatives* (FN) are those which are labeled 'negative' which are actually 'positive'\n",
    "\n",
    "We can do this by creating a confusion matrix for the results. The result is a NumPy matrix, with predictions on the columns and actual labels on the rows. The values correspond to:\n",
    "\n",
    "    [ [TP FN]\n",
    "    [FP TN] ]\n",
    "A perfect classifier with 100% accuracy would produce a pure diagonal matrix which would have all the test examples predicted in their correct class. In our case, we see that we have many false negatives (i.e. examples labelled -1 which are actually 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# build the confusion matrix\n",
    "cm = confusion_matrix(target_test, predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can produce a more visual version of this matrix using the *ConfusionMatrixDisplay* class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# note we get the labels (names) for the classes from the classifier\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures from information retrieval (search engines) can be used in ML evaluation. Note that these are calculated with respect to a particular class (e.g. positive class or the negative class):\n",
    "- *Precision*: proportion of retrieved results that are relevant = TP/(TP+FP)\n",
    "- *Recall*: proportion of relevant results that are retrieved = TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "# indicate that we are interested in the Positive class here\n",
    "print(\"Precision(Positive) = %.3f\" % precision_score(target_test, predicted, pos_label=\"positive\") )\n",
    "print(\"Recall(Positive) = %.3f\" % recall_score(target_test, predicted, pos_label=\"positive\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively indicate that we are interested in the Negative class here\n",
    "print(\"Precision(Negative) = %.3f\" % precision_score(target_test, predicted, pos_label=\"negative\") )\n",
    "print(\"Recall(Negative) = %.3f\" % recall_score(target_test, predicted, pos_label=\"negative\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is often a trade-off between precision and recall. We can combine precision and recall into a single score using the *F1 Measure*, which is a weighted average of the precision and recall. The F1 Measure reaches its best value at 1 and worst at 0. Again this score is calculate with respect to a specified class (e.g. positive or negative)\n",
    "\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"F1(Positive) = %.3f\" % f1_score(target_test, predicted, pos_label=\"positive\") )\n",
    "print(\"F1(Negative) = %.3f\" % f1_score(target_test, predicted, pos_label=\"negative\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly compute a summary of these statistics using scikit-learn's provided convenience function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# note we get the labels (names) for the classes from the classifier\n",
    "print(classification_report(target_test, predicted, target_names=knn.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "A problem with simply randomly splitting a dataset into two sets is that each random split might give different results. We are also ignoring a portion of your dataset. One way to address this is to use *K-fold cross-validation* to evaluate a classifier:\n",
    "1. Divide the data into K disjoint subsets - “folds” (e.g. K=5 folds).\n",
    "2. For each of K experiments, use K-1 folds for training and the selected one fold for testing.\n",
    "3. Repeat for all K folds, average the accuracy/error rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is a relatively complex process, scikit-learn allows us to achieve this using a single function *cross_val_score()*. Let's do a 2-fold cross-validation of the KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# apply 2-fold cross-validation, measuring accuracy each time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# the argument 'cv' specifies the number of folds to use\n",
    "acc_scores = cross_val_score(knn, data_scaled, target, cv=2, scoring=\"accuracy\")\n",
    "print(acc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for 5-fold cross validation we get an array with 5 accuracy scores, one score for each fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = cross_val_score(knn, data_scaled, target, cv=5, scoring=\"accuracy\")\n",
    "print(acc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we calculate the average accuracy across all folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KNN (k=3): Mean cross-validation accuracy = %.3f\" % acc_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this approach to compare different classification algorithms on the same data, such as a logistic regression classifier or a Support Vector Machine (SVM) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression(solver='lbfgs')\n",
    "acc_scores = cross_val_score(lr, data_scaled, target, cv=5, scoring=\"accuracy\")\n",
    "print(\"Logistic Regression: Mean cross-validation accuracy = %.3f\" % acc_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(gamma='auto')\n",
    "acc_scores = cross_val_score(svm, data_scaled, target, cv=5, scoring=\"accuracy\")\n",
    "print(\"SVM: Mean cross-validation accuracy = %.3f\" % acc_scores.mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we could use cross-validation as part of an approach to select parameters for a classifcation algorithm, such as the number of neighbours *k* for a KNN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over a range of values of k\n",
    "for k in range(1, 11):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    acc_scores = cross_val_score(knn, data_scaled, target, cv=5, scoring=\"accuracy\")\n",
    "    mean_acc = acc_scores.mean()\n",
    "    print(\"K=%02d neighbours: Accuracy=%.3f\" % (k, mean_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
