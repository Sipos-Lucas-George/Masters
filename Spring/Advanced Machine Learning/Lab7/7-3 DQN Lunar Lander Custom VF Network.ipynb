{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590 - Advanced Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN for Lunar Lander - Custom Value Network\n",
    "Uses a Deep Q Network to train a neural network based player for the Lunar Lander environment from Gymnasium (https://gymnasium.farama.org/environments/box2d/lunar_lander/). This uses a vector-based state representation and uses a custom value network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Google colab you need to isntall packages  - comment out lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt install swig cmake ffmpeg\n",
    "#!apt-get install -y xvfb x11-utils\n",
    "#!python -m pip install 'git+https://github.com/DLR-RM/stable-baselines3@feat/gymnasium-support#egg=stable-baselines3[extra]' \n",
    "#!pip install pyglet box2d box2d-kengz\n",
    "#!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Google colab comment out this cell to make a virtual rendering canvas so render calls work (we still wont; see display!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyvirtualdisplay\n",
    "#\n",
    "#_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "#                                    size=(1400, 900))\n",
    "#_ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 22:03:46.818235: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import stable_baselines3 as sb3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Lunar Lander Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env_train = gym.make('LunarLander-v2')\n",
    "env_train = gym.wrappers.TimeLimit(env_train, max_episode_steps=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a basic DQN agent and inspect its networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNPolicy(\n",
      "  (q_net): QNetwork(\n",
      "    (features_extractor): FlattenExtractor(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (q_net_target): QNetwork(\n",
      "    (features_extractor): FlattenExtractor(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agent = sb3.DQN('MlpPolicy', \n",
    "                env_train)\n",
    "print(agent.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for a custom value-function network in the DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_network_args = {'activation_fn':torch.nn.ReLU,\n",
    "                   'net_arch':[256, 256]}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the agent using the custom value function network architecture. Also change some hyperparameters to tuned values:\n",
    "- learning_rate: 0.00063\n",
    "- batch_size: 128\n",
    "- buffer_size: 50000\n",
    "- learning_starts: 0\n",
    "- target_update_interval: 250\n",
    "- gradient_steps: -1\n",
    "- exploration_fraction: 0.12\n",
    "- exploration_final_eps: 0.1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "tb_log = './log_tb_lunarlander/'\n",
    "agent = sb3.DQN('MlpPolicy', \n",
    "                env_train, \n",
    "                learning_rate = 0.00063,\n",
    "                batch_size = 128,\n",
    "                buffer_size = 50000,\n",
    "                learning_starts = 0,\n",
    "                target_update_interval = 250,\n",
    "                gradient_steps = -1,\n",
    "                exploration_fraction = 0.12,\n",
    "                exploration_final_eps = 0.1,\n",
    "                verbose=1, \n",
    "                policy_kwargs = vf_network_args,\n",
    "                tensorboard_log=tb_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the vf networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNPolicy(\n",
      "  (q_net): QNetwork(\n",
      "    (features_extractor): FlattenExtractor(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=8, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=256, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (q_net_target): QNetwork(\n",
      "    (features_extractor): FlattenExtractor(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=8, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=256, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(agent.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an evaluation callback with a long wait between steps and no rendering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make('LunarLander-v2', render_mode = 'human') # We use a separate evaluation env in case any wrappers have been used\n",
    "eval_callback = sb3.common.callbacks.EvalCallback(eval_env, \n",
    "                                                  best_model_save_path='./logs_lunarlander_custom/',\n",
    "                                                  log_path='./logs_lunarlander_custom/', \n",
    "                                                  eval_freq=5000,\n",
    "                                                  render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent for a large number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./log_tb_lunarlander/Custom Network_6\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 80       |\n",
      "|    ep_rew_mean      | -85.5    |\n",
      "|    exploration_rate | 0.988    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 125      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 320      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.77     |\n",
      "|    n_updates        | 316      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 91.5     |\n",
      "|    ep_rew_mean      | -109     |\n",
      "|    exploration_rate | 0.973    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 732      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.34     |\n",
      "|    n_updates        | 728      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.1     |\n",
      "|    ep_rew_mean      | -138     |\n",
      "|    exploration_rate | 0.958    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 142      |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 1117     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.17     |\n",
      "|    n_updates        | 1116     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.8     |\n",
      "|    ep_rew_mean      | -140     |\n",
      "|    exploration_rate | 0.947    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 153      |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 1420     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.61     |\n",
      "|    n_updates        | 1416     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.4     |\n",
      "|    ep_rew_mean      | -135     |\n",
      "|    exploration_rate | 0.933    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 157      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 1788     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.17     |\n",
      "|    n_updates        | 1784     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.4     |\n",
      "|    ep_rew_mean      | -125     |\n",
      "|    exploration_rate | 0.92     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 157      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 2145     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 2144     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88       |\n",
      "|    ep_rew_mean      | -123     |\n",
      "|    exploration_rate | 0.908    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 159      |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 2465     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.24     |\n",
      "|    n_updates        | 2464     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.3     |\n",
      "|    ep_rew_mean      | -125     |\n",
      "|    exploration_rate | 0.894    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 158      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 2825     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.23     |\n",
      "|    n_updates        | 2824     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.4     |\n",
      "|    ep_rew_mean      | -118     |\n",
      "|    exploration_rate | 0.881    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 159      |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 3182     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.25     |\n",
      "|    n_updates        | 3180     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.8     |\n",
      "|    ep_rew_mean      | -116     |\n",
      "|    exploration_rate | 0.864    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 158      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 3633     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.99     |\n",
      "|    n_updates        | 3632     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.5     |\n",
      "|    ep_rew_mean      | -116     |\n",
      "|    exploration_rate | 0.851    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 160      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 3984     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.62     |\n",
      "|    n_updates        | 3980     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.8     |\n",
      "|    ep_rew_mean      | -114     |\n",
      "|    exploration_rate | 0.837    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 159      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 4357     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.45     |\n",
      "|    n_updates        | 4356     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90       |\n",
      "|    ep_rew_mean      | -113     |\n",
      "|    exploration_rate | 0.824    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 161      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 4682     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.53     |\n",
      "|    n_updates        | 4680     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmacnamee/opt/anaconda3/envs/COMP47590_2024/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "2025-03-27 22:04:31.220 python3.12[7172:231903] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-27 22:04:31.220 python3.12[7172:231903] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=-114.41 +/- 30.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -114     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.813    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.66     |\n",
      "|    n_updates        | 4996     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.5     |\n",
      "|    ep_rew_mean      | -111     |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 37       |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 5068     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.22     |\n",
      "|    n_updates        | 5064     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.6     |\n",
      "|    ep_rew_mean      | -108     |\n",
      "|    exploration_rate | 0.796    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 39       |\n",
      "|    time_elapsed     | 138      |\n",
      "|    total_timesteps  | 5437     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.786    |\n",
      "|    n_updates        | 5436     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 91.1     |\n",
      "|    ep_rew_mean      | -105     |\n",
      "|    exploration_rate | 0.781    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 41       |\n",
      "|    time_elapsed     | 140      |\n",
      "|    total_timesteps  | 5832     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.28     |\n",
      "|    n_updates        | 5828     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 91.7     |\n",
      "|    ep_rew_mean      | -106     |\n",
      "|    exploration_rate | 0.766    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 43       |\n",
      "|    time_elapsed     | 143      |\n",
      "|    total_timesteps  | 6238     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.2      |\n",
      "|    n_updates        | 6236     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.5     |\n",
      "|    ep_rew_mean      | -104     |\n",
      "|    exploration_rate | 0.75     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 45       |\n",
      "|    time_elapsed     | 145      |\n",
      "|    total_timesteps  | 6658     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.51     |\n",
      "|    n_updates        | 6656     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.4     |\n",
      "|    ep_rew_mean      | -101     |\n",
      "|    exploration_rate | 0.737    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 148      |\n",
      "|    total_timesteps  | 7024     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 7020     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.1     |\n",
      "|    ep_rew_mean      | -98.8    |\n",
      "|    exploration_rate | 0.721    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 49       |\n",
      "|    time_elapsed     | 150      |\n",
      "|    total_timesteps  | 7448     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.07     |\n",
      "|    n_updates        | 7444     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.1     |\n",
      "|    ep_rew_mean      | -97.6    |\n",
      "|    exploration_rate | 0.704    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 153      |\n",
      "|    total_timesteps  | 7902     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.989    |\n",
      "|    n_updates        | 7900     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.5     |\n",
      "|    ep_rew_mean      | -95.6    |\n",
      "|    exploration_rate | 0.685    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 156      |\n",
      "|    total_timesteps  | 8405     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 8404     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.1     |\n",
      "|    ep_rew_mean      | -94.5    |\n",
      "|    exploration_rate | 0.672    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 158      |\n",
      "|    total_timesteps  | 8745     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.47     |\n",
      "|    n_updates        | 8744     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.6     |\n",
      "|    ep_rew_mean      | -93.6    |\n",
      "|    exploration_rate | 0.659    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 56       |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 9084     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 9080     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.6     |\n",
      "|    ep_rew_mean      | -92.5    |\n",
      "|    exploration_rate | 0.645    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 58       |\n",
      "|    time_elapsed     | 161      |\n",
      "|    total_timesteps  | 9456     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.34     |\n",
      "|    n_updates        | 9452     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.1     |\n",
      "|    ep_rew_mean      | -91      |\n",
      "|    exploration_rate | 0.631    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 164      |\n",
      "|    total_timesteps  | 9829     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.11     |\n",
      "|    n_updates        | 9828     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-335.23 +/- 264.34\n",
      "Episode length: 322.80 +/- 210.72\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 323      |\n",
      "|    mean_reward      | -335     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.625    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.04     |\n",
      "|    n_updates        | 9996     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.9     |\n",
      "|    ep_rew_mean      | -90      |\n",
      "|    exploration_rate | 0.617    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 199      |\n",
      "|    total_timesteps  | 10220    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.06     |\n",
      "|    n_updates        | 10216    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.9     |\n",
      "|    ep_rew_mean      | -84.2    |\n",
      "|    exploration_rate | 0.602    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 202      |\n",
      "|    total_timesteps  | 10608    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.832    |\n",
      "|    n_updates        | 10604    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.6     |\n",
      "|    ep_rew_mean      | -80.7    |\n",
      "|    exploration_rate | 0.588    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 205      |\n",
      "|    total_timesteps  | 10976    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.826    |\n",
      "|    n_updates        | 10972    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.4     |\n",
      "|    ep_rew_mean      | -77.6    |\n",
      "|    exploration_rate | 0.575    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 208      |\n",
      "|    total_timesteps  | 11328    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.24     |\n",
      "|    n_updates        | 11324    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.7     |\n",
      "|    ep_rew_mean      | -76.9    |\n",
      "|    exploration_rate | 0.561    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 210      |\n",
      "|    total_timesteps  | 11711    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.954    |\n",
      "|    n_updates        | 11708    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 96.3     |\n",
      "|    ep_rew_mean      | -74.7    |\n",
      "|    exploration_rate | 0.546    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 56       |\n",
      "|    time_elapsed     | 212      |\n",
      "|    total_timesteps  | 12094    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.922    |\n",
      "|    n_updates        | 12092    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 97.5     |\n",
      "|    ep_rew_mean      | -71.5    |\n",
      "|    exploration_rate | 0.528    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 58       |\n",
      "|    time_elapsed     | 214      |\n",
      "|    total_timesteps  | 12579    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.751    |\n",
      "|    n_updates        | 12576    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 97.5     |\n",
      "|    ep_rew_mean      | -71.3    |\n",
      "|    exploration_rate | 0.515    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 216      |\n",
      "|    total_timesteps  | 12934    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.621    |\n",
      "|    n_updates        | 12932    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.3     |\n",
      "|    ep_rew_mean      | -70.9    |\n",
      "|    exploration_rate | 0.495    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 219      |\n",
      "|    total_timesteps  | 13462    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.35     |\n",
      "|    n_updates        | 13460    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -67.7    |\n",
      "|    exploration_rate | 0.475    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 222      |\n",
      "|    total_timesteps  | 14002    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.34     |\n",
      "|    n_updates        | 14000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -72.3    |\n",
      "|    exploration_rate | 0.441    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 227      |\n",
      "|    total_timesteps  | 14909    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.17     |\n",
      "|    n_updates        | 14908    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-202.11 +/- 64.42\n",
      "Episode length: 485.00 +/- 270.30\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 485      |\n",
      "|    mean_reward      | -202     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.438    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.41     |\n",
      "|    n_updates        | 14996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 109      |\n",
      "|    ep_rew_mean      | -69.8    |\n",
      "|    exploration_rate | 0.417    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 281      |\n",
      "|    total_timesteps  | 15552    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.81     |\n",
      "|    n_updates        | 15548    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 111      |\n",
      "|    ep_rew_mean      | -68.6    |\n",
      "|    exploration_rate | 0.393    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 56       |\n",
      "|    time_elapsed     | 284      |\n",
      "|    total_timesteps  | 16200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 16196    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | -66.1    |\n",
      "|    exploration_rate | 0.376    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 58       |\n",
      "|    time_elapsed     | 286      |\n",
      "|    total_timesteps  | 16648    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.18     |\n",
      "|    n_updates        | 16644    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 117      |\n",
      "|    ep_rew_mean      | -67.4    |\n",
      "|    exploration_rate | 0.342    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 290      |\n",
      "|    total_timesteps  | 17544    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.962    |\n",
      "|    n_updates        | 17540    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 121      |\n",
      "|    ep_rew_mean      | -66.1    |\n",
      "|    exploration_rate | 0.313    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 296      |\n",
      "|    total_timesteps  | 18323    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.33     |\n",
      "|    n_updates        | 18320    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-68.94 +/- 6.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -68.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.25     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.21     |\n",
      "|    n_updates        | 19996    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 148      |\n",
      "|    ep_rew_mean      | -70.2    |\n",
      "|    exploration_rate | 0.195    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 418      |\n",
      "|    total_timesteps  | 21458    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.09     |\n",
      "|    n_updates        | 21456    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 176      |\n",
      "|    ep_rew_mean      | -68.8    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 447      |\n",
      "|    total_timesteps  | 24622    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.75     |\n",
      "|    n_updates        | 24620    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-204.61 +/- 202.35\n",
      "Episode length: 931.20 +/- 94.03\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 931      |\n",
      "|    mean_reward      | -205     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.61     |\n",
      "|    n_updates        | 24996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 212      |\n",
      "|    ep_rew_mean      | -66.7    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 569      |\n",
      "|    total_timesteps  | 28622    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.09     |\n",
      "|    n_updates        | 28620    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=187.35 +/- 175.08\n",
      "Episode length: 360.00 +/- 137.79\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 360      |\n",
      "|    mean_reward      | 187      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.97     |\n",
      "|    n_updates        | 29996    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 238      |\n",
      "|    ep_rew_mean      | -64      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 627      |\n",
      "|    total_timesteps  | 31744    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.8      |\n",
      "|    n_updates        | 31740    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 256      |\n",
      "|    ep_rew_mean      | -54.1    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 640      |\n",
      "|    total_timesteps  | 33979    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.85     |\n",
      "|    n_updates        | 33976    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=85.13 +/- 114.67\n",
      "Episode length: 798.60 +/- 247.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 799      |\n",
      "|    mean_reward      | 85.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 35000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.63     |\n",
      "|    n_updates        | 34996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 266      |\n",
      "|    ep_rew_mean      | -43.2    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 48       |\n",
      "|    time_elapsed     | 730      |\n",
      "|    total_timesteps  | 35334    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.52     |\n",
      "|    n_updates        | 35332    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 283      |\n",
      "|    ep_rew_mean      | -35      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 740      |\n",
      "|    total_timesteps  | 37362    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3        |\n",
      "|    n_updates        | 37360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=50.07 +/- 213.53\n",
      "Episode length: 545.60 +/- 246.46\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 546      |\n",
      "|    mean_reward      | 50.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.31     |\n",
      "|    n_updates        | 39996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 310      |\n",
      "|    ep_rew_mean      | -25.8    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 49       |\n",
      "|    time_elapsed     | 825      |\n",
      "|    total_timesteps  | 40439    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.91     |\n",
      "|    n_updates        | 40436    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 327      |\n",
      "|    ep_rew_mean      | -13.3    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 835      |\n",
      "|    total_timesteps  | 42511    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.63     |\n",
      "|    n_updates        | 42508    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=49.04 +/- 63.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 49       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 45000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.12     |\n",
      "|    n_updates        | 44996    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 360      |\n",
      "|    ep_rew_mean      | -3.73    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 48       |\n",
      "|    time_elapsed     | 961      |\n",
      "|    total_timesteps  | 46222    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.65     |\n",
      "|    n_updates        | 46220    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=205.57 +/- 41.67\n",
      "Episode length: 618.40 +/- 233.92\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 618      |\n",
      "|    mean_reward      | 206      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.63     |\n",
      "|    n_updates        | 49996    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 394      |\n",
      "|    ep_rew_mean      | 4.58     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 1045     |\n",
      "|    total_timesteps  | 50017    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.4      |\n",
      "|    n_updates        | 50016    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 419      |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 49       |\n",
      "|    time_elapsed     | 1058     |\n",
      "|    total_timesteps  | 52862    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.8      |\n",
      "|    n_updates        | 52860    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=215.13 +/- 48.22\n",
      "Episode length: 623.40 +/- 120.15\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 623      |\n",
      "|    mean_reward      | 215      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 55000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.46     |\n",
      "|    n_updates        | 54996    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 447      |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 49       |\n",
      "|    time_elapsed     | 1137     |\n",
      "|    total_timesteps  | 56077    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 56076    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 478      |\n",
      "|    ep_rew_mean      | 31.4     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 1152     |\n",
      "|    total_timesteps  | 59480    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.47     |\n",
      "|    n_updates        | 59476    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=230.31 +/- 17.49\n",
      "Episode length: 376.80 +/- 102.92\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 377      |\n",
      "|    mean_reward      | 230      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.56     |\n",
      "|    n_updates        | 59996    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 501      |\n",
      "|    ep_rew_mean      | 42.6     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 1204     |\n",
      "|    total_timesteps  | 62243    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.24     |\n",
      "|    n_updates        | 62240    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 522      |\n",
      "|    ep_rew_mean      | 54.5     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 1217     |\n",
      "|    total_timesteps  | 64828    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 64824    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "agent.learn(total_timesteps=200000, \n",
    "            tb_log_name=\"Custom Network\",\n",
    "           callback = eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"./dqn_lunar_lander_agent_custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the agent in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent, \n",
    "                                                                eval_env, \n",
    "                                                                n_eval_episodes=10,\n",
    "                                                               render = True)\n",
    "print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best model seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent = sb3.dqn.DQN.load(\"./logs_lunarlander_custom/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(best_agent, \n",
    "                                                                eval_env, \n",
    "                                                                n_eval_episodes=10,\n",
    "                                                               render = True)\n",
    "print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261.818px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
