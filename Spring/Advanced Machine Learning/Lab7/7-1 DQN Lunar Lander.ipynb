{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590 - Advanced Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN for Lunar Lander\n",
    "Uses a Deep Q Network to train a neural network based player for the Lunar Lander environment from Gymnasium (https://gymnasium.farama.org/environments/box2d/lunar_lander/).. This uses a vector-based state representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lunar Lander](lunar_lander.gif)\n",
    "\n",
    "There are four **actions** in this environment:\n",
    "- none (0)\n",
    "- left engine (1)\n",
    "- main engine (2)\n",
    "- right engine (3)\n",
    "\n",
    "**Reward** is awarded after each frame as follows:\n",
    "- crash: -100 \n",
    "- land: +100 \n",
    "- leg ground contact: +10\n",
    "- firing main engine: -0.3\n",
    "- landing between flags: +200\n",
    "\n",
    "And the **state** is represented using 8 values:\n",
    "- position of the spaceship (in x and y coordinates) \n",
    "- the velocity of the spaceship (in x and y directions), \n",
    "- the angular velocity of the spaceship\n",
    "- the angle of the line connecting the spaceship to the landing pad\n",
    "- leg contact with ground (left and right)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation - Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Google colab you need to install packages  - comment out lines below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.471136Z",
     "start_time": "2025-04-03T13:36:40.468800Z"
    }
   },
   "source": [
    "#!apt install swig cmake \n",
    "#!apt-get install -y xvfb x11-utils\n",
    "#!apt-get install -y python-opengl ffmpeg > /dev/null 2>&1\n",
    "#!python -m pip install 'git+https://github.com/DLR-RM/stable-baselines3@feat/gymnasium-support#egg=stable-baselines3[extra]' \n",
    "#!pip install pyglet box2d box2d-kengz\n",
    "#!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate\n",
    "#!pip install -U colabgymrender"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Google colab comment out this cell to make a virtual rendering canvas so render calls work (we still wont; see display!)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.478599Z",
     "start_time": "2025-04-03T13:36:40.476299Z"
    }
   },
   "source": [
    "#import pyvirtualdisplay\n",
    "#\n",
    "#_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "#                                    size=(1400, 900))\n",
    "#_ = _display.start()"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.490976Z",
     "start_time": "2025-04-03T13:36:40.489356Z"
    }
   },
   "source": [
    "#from colabgymrender.recorder import Recorder\n",
    "#env = Recorder(env,\"./video\")\n",
    "#\n",
    "#obs = env.reset()\n",
    "#done = False\n",
    "#while not done:\n",
    "#    action = env.action_space.sample()\n",
    "#    obs, reward, done, info = env.step(action)\n",
    "#\n",
    "#env.play()"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation - Windows\n",
    "To setup on a Windows machine you need the following pre-requisites.\n",
    "* git to be installed (https://git-scm.com/download/win) and be on the path\n",
    "* Visual Studio Developer Tools to be installed (https://visualstudio.microsoft.com/visual-cpp-build-tools/)\n",
    "\n",
    "Uncomment and run the following:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.495529Z",
     "start_time": "2025-04-03T13:36:40.494057Z"
    }
   },
   "source": [
    "#!pip install tqdm rich\n",
    "#!pip install pytorch\n",
    "#!pip install gymnasium\n",
    "#!python -m pip install 'git+https://github.com/DLR-RM/stable-baselines3@feat/gymnasium-support#egg=stable-baselines3[extra]'\n",
    "#!pip install pygame\n",
    "#!pip install pyglet"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation - Mac\n",
    "Uncomment and run the following:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.501418Z",
     "start_time": "2025-04-03T13:36:40.500088Z"
    }
   },
   "source": [
    "# !pip install tqdm rich\n",
    "# !pip install ipywidgets\n",
    "# !pip install pytorch\n",
    "# !pip install gymnasium\n",
    "# !pip install stable-baselines3\n",
    "# !pip install pygame\n",
    "# !pip install pyglet\n",
    "# !conda install swig -y # needed to build Box2D in the pip install"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.506257Z",
     "start_time": "2025-04-03T13:36:40.504934Z"
    }
   },
   "source": "# !xcode-select --install",
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.511338Z",
     "start_time": "2025-04-03T13:36:40.509974Z"
    }
   },
   "source": "# !pip install box2d-py # a repackaged version of pybox2d",
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.516062Z",
     "start_time": "2025-04-03T13:36:40.514855Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import stable_baselines3 as sb3"
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Lunar Lander Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.521417Z",
     "start_time": "2025-04-03T13:36:40.519664Z"
    }
   },
   "source": [
    "env_render = gym.make('LunarLander-v2', \n",
    "               render_mode = 'human')"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a time limit wrapper to avoid infinite hovering (remember the spaceship never runs out of fuel!)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.526365Z",
     "start_time": "2025-04-03T13:36:40.524821Z"
    }
   },
   "source": [
    "env_render = gym.wrappers.TimeLimit(env_render, \n",
    "                                    max_episode_steps = 3000)"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the Lunar Lander environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.532812Z",
     "start_time": "2025-04-03T13:36:40.531022Z"
    }
   },
   "source": [
    "env_render.action_space"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.546790Z",
     "start_time": "2025-04-03T13:36:40.544307Z"
    }
   },
   "source": [
    "env_render.observation_space"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-90.        -90.         -5.         -5.         -3.1415927  -5.\n",
       "  -0.         -0.       ], [90.        90.         5.         5.         3.1415927  5.\n",
       "  1.         1.       ], (8,), float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:40.656093Z",
     "start_time": "2025-04-03T13:36:40.564534Z"
    }
   },
   "source": [
    "env_render.reset()\n",
    "env_render.render()"
   ],
   "outputs": [],
   "execution_count": 85
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play an episode of the Lunar Lander environment using random actions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:43.376879Z",
     "start_time": "2025-04-03T13:36:40.660577Z"
    }
   },
   "source": [
    "obs, _ = env_render.reset()\n",
    "\n",
    "terminate = False\n",
    "truncate = False\n",
    "\n",
    "while not (terminate or truncate):\n",
    "    \n",
    "    action = env_render.action_space.sample()\n",
    "    obs, reward, terminate, truncate, info = env_render.step(action)\n",
    "    \n",
    "    env_render.render()"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete an episode of the Lunar Lander environment using random actions recording actions and reward."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:43.394346Z",
     "start_time": "2025-04-03T13:36:43.390705Z"
    }
   },
   "source": [
    "cumulative_reward = 0\n",
    "actions = []\n",
    "action_map = {0:'none', \n",
    "            1:'left engine',\n",
    "            2:'main engine',\n",
    "            3:'right engine'}"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:47.201936Z",
     "start_time": "2025-04-03T13:36:43.402466Z"
    }
   },
   "source": [
    "obs, _ = env_render.reset()\n",
    "\n",
    "terminate = False\n",
    "truncate = False\n",
    "while not (terminate or truncate):\n",
    "    \n",
    "    action = env_render.action_space.sample()\n",
    "    obs, reward, terminate, truncate, info = env_render.step(action)\n",
    "    print(reward)\n",
    "    \n",
    "    # Record reward and action\n",
    "    cumulative_reward = cumulative_reward + reward\n",
    "    actions.append(action)\n",
    "    \n",
    "    env_render.render()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5788542607556895\n",
      "-2.4488270692613683\n",
      "-2.25583937776031\n",
      "-3.230851275361515\n",
      "-2.256738861297491\n",
      "2.2605112261959563\n",
      "-3.0373441199956788\n",
      "-2.003048836411607\n",
      "2.5539433404101546\n",
      "-1.6484483536123935\n",
      "-1.819216733804011\n",
      "-2.5305333347310452\n",
      "-2.183158218676084\n",
      "-2.7679098495373453\n",
      "-2.3041821150175963\n",
      "-2.723861046043082\n",
      "-2.3677161770833095\n",
      "-2.81528874664471\n",
      "3.1282038383383624\n",
      "-2.582712679385452\n",
      "-3.0250226408456045\n",
      "-3.178131377972447\n",
      "3.050467520654178\n",
      "1.0966834168944615\n",
      "-3.6176206125942643\n",
      "-2.3616791528446854\n",
      "-2.2699469411498954\n",
      "-2.530481685384501\n",
      "-2.480865455626656\n",
      "-2.4296459780397015\n",
      "-3.1162841228114346\n",
      "-2.559213984493539\n",
      "1.0302307006622697\n",
      "-2.1220612238268175\n",
      "-2.3199071665100632\n",
      "-2.7928863752097173\n",
      "-1.7283027191281792\n",
      "1.6119041212866534\n",
      "-2.6633455798106227\n",
      "1.6446961770311759\n",
      "-1.641735723558013\n",
      "1.7895555904297964\n",
      "-2.7945675818376103\n",
      "-1.3880165098927637\n",
      "-2.586947948604744\n",
      "1.2893001964395807\n",
      "-1.2064143684697786\n",
      "-2.3183005902708644\n",
      "-2.551819001308472\n",
      "-2.744788076379136\n",
      "-1.3859275807471068\n",
      "-1.0512185126062252\n",
      "-1.5725902951683395\n",
      "-1.5216173567391422\n",
      "-2.413521267571524\n",
      "-1.0823627749919342\n",
      "-1.4534338907680535\n",
      "0.21933695644700607\n",
      "-1.356686734267356\n",
      "-0.6574785541532993\n",
      "-1.7474113052817575\n",
      "-1.2019039954820414\n",
      "2.6248234391376686\n",
      "1.6055939652357665\n",
      "1.4686461526592154\n",
      "-0.6215117307322646\n",
      "-1.1390324080300616\n",
      "-0.2727283639347593\n",
      "2.77774079093889\n",
      "-1.6868778824821493\n",
      "1.125167423115738\n",
      "-2.0849685524157153\n",
      "-1.3081628904812135\n",
      "-0.4597820676652862\n",
      "-0.344795355948462\n",
      "-0.24359642393258582\n",
      "0.07258259495788025\n",
      "0.5093792032785427\n",
      "0.48228391714928986\n",
      "0.780456582667399\n",
      "0.06226568616150985\n",
      "-0.0279358958793523\n",
      "0.4758689883124998\n",
      "1.4832809732016414\n",
      "-0.484197295737971\n",
      "2.9302655034725946\n",
      "0.845442382353508\n",
      "8.166277995116616\n",
      "15.567572799600413\n",
      "-100\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print actions, rewards and cumulative reward."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:47.214633Z",
     "start_time": "2025-04-03T13:36:47.211618Z"
    }
   },
   "source": [
    "print(\"Actions: \", ', '.join([action_map[a] for a in actions]))\n",
    "print(\"Cumulative Reward: {}\".format(cumulative_reward))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  right engine, right engine, none, right engine, left engine, main engine, right engine, left engine, main engine, left engine, left engine, right engine, none, right engine, none, right engine, none, right engine, main engine, none, right engine, right engine, main engine, main engine, right engine, left engine, left engine, none, none, none, right engine, none, main engine, left engine, none, right engine, left engine, main engine, right engine, main engine, left engine, main engine, right engine, left engine, right engine, main engine, left engine, right engine, right engine, right engine, left engine, left engine, none, none, right engine, left engine, none, main engine, none, left engine, right engine, none, main engine, main engine, main engine, left engine, none, left engine, main engine, right engine, main engine, right engine, none, left engine, left engine, left engine, left engine, left engine, left engine, left engine, none, none, left engine, main engine, none, main engine, main engine, none, none, main engine\n",
      "Cumulative Reward: -159.44777552481602\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an environment without rendering."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:47.233542Z",
     "start_time": "2025-04-03T13:36:47.230229Z"
    }
   },
   "source": "env_train = gym.make('LunarLander-v2')",
   "outputs": [],
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple DQN agent using stable-baselines3. LunarLander uses a state vector representation so a simple MLP can drive this model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:47.262610Z",
     "start_time": "2025-04-03T13:36:47.252737Z"
    }
   },
   "source": [
    "agent = sb3.DQN('MlpPolicy', \n",
    "                env_train, \n",
    "                verbose=1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent for a large number of steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:48.421833Z",
     "start_time": "2025-04-03T13:36:47.418119Z"
    }
   },
   "source": [
    "agent.learn(total_timesteps=5000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 97       |\n",
      "|    ep_rew_mean      | -501     |\n",
      "|    exploration_rate | 0.263    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 5380     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 388      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.73     |\n",
      "|    n_updates        | 71       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 86.8     |\n",
      "|    ep_rew_mean      | -434     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 4963     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 694      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.08     |\n",
      "|    n_updates        | 148      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 116      |\n",
      "|    ep_rew_mean      | -427     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 4789     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1390     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.1      |\n",
      "|    n_updates        | 322      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -418     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 4803     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1669     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.51     |\n",
      "|    n_updates        | 392      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 97.1     |\n",
      "|    ep_rew_mean      | -412     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 4829     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1942     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.6      |\n",
      "|    n_updates        | 460      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.7     |\n",
      "|    ep_rew_mean      | -380     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 4857     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2248     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.85     |\n",
      "|    n_updates        | 536      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.8     |\n",
      "|    ep_rew_mean      | -361     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 4895     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2599     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 624      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.4     |\n",
      "|    ep_rew_mean      | -326     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 4917     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2894     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.63     |\n",
      "|    n_updates        | 698      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.7     |\n",
      "|    ep_rew_mean      | -306     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 4942     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3228     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.14     |\n",
      "|    n_updates        | 781      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 87.5     |\n",
      "|    ep_rew_mean      | -289     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 4957     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3500     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.04     |\n",
      "|    n_updates        | 849      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 85.5     |\n",
      "|    ep_rew_mean      | -274     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 4961     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3762     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.959    |\n",
      "|    n_updates        | 915      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 84.8     |\n",
      "|    ep_rew_mean      | -264     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 4978     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4072     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.04     |\n",
      "|    n_updates        | 992      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 83.2     |\n",
      "|    ep_rew_mean      | -253     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 4983     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4326     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.971    |\n",
      "|    n_updates        | 1056     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 82.7     |\n",
      "|    ep_rew_mean      | -245     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 4996     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4631     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.723    |\n",
      "|    n_updates        | 1132     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 81.2     |\n",
      "|    ep_rew_mean      | -237     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 4996     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4871     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.96     |\n",
      "|    n_updates        | 1192     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x33dfff020>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the agent in the environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:37:03.077716Z",
     "start_time": "2025-04-03T13:36:48.434436Z"
    }
   },
   "source": [
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent, \n",
    "                                                                env_render,\n",
    "                                                                render = True,\n",
    "                                                                n_eval_episodes=10)\n",
    "print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/AI/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:259: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -140.84506327623967 +/- 17.65738118891108\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save an agent easily in SB3."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:37:03.111659Z",
     "start_time": "2025-04-03T13:37:03.100568Z"
    }
   },
   "source": [
    "agent.save(\"./dqn_lunar_lander_agent\")"
   ],
   "outputs": [],
   "execution_count": 94
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily load an agent. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:37:03.135086Z",
     "start_time": "2025-04-03T13:37:03.126042Z"
    }
   },
   "source": [
    "agent = sb3.dqn.DQN.load(\"./dqn_lunar_lander_agent\")"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the agent into the environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:37:06.267952Z",
     "start_time": "2025-04-03T13:37:03.139830Z"
    }
   },
   "source": [
    "obs, _ = env_render.reset()\n",
    "\n",
    "terminate = False\n",
    "truncate = False\n",
    "while not (terminate or truncate):\n",
    "\n",
    "    action, _ = agent.predict(obs, deterministic = True)\n",
    "    \n",
    "    obs, reward, terminate, truncate, _ = env_render.step(action)\n",
    "    \n",
    "    env_render.render()"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also continue training a loaded agent. First set the environment (this is not saved with the agent)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:37:06.274080Z",
     "start_time": "2025-04-03T13:37:06.272050Z"
    }
   },
   "source": [
    "agent.set_env(env_train)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now continue training the agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-03T13:37:08.322789Z",
     "start_time": "2025-04-03T13:37:06.299640Z"
    }
   },
   "source": [
    "agent.learn(total_timesteps = 10000, \n",
    "            reset_num_timesteps = False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 80.1     |\n",
      "|    ep_rew_mean      | -230     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 3965     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5132     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.188    |\n",
      "|    n_updates        | 1257     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 79.9     |\n",
      "|    ep_rew_mean      | -226     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 4376     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5436     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.725    |\n",
      "|    n_updates        | 1333     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 79.5     |\n",
      "|    ep_rew_mean      | -222     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 4645     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5727     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.744    |\n",
      "|    n_updates        | 1406     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 78.7     |\n",
      "|    ep_rew_mean      | -218     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 4755     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5985     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.314    |\n",
      "|    n_updates        | 1471     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 78.1     |\n",
      "|    ep_rew_mean      | -211     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 4847     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6250     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.5      |\n",
      "|    n_updates        | 1537     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 77.3     |\n",
      "|    ep_rew_mean      | -207     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 4913     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6497     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.23     |\n",
      "|    n_updates        | 1599     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 76.6     |\n",
      "|    ep_rew_mean      | -203     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 4961     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6740     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.539    |\n",
      "|    n_updates        | 1659     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 76.4     |\n",
      "|    ep_rew_mean      | -202     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 5005     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7028     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.925    |\n",
      "|    n_updates        | 1731     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 76       |\n",
      "|    ep_rew_mean      | -199     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 5026     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7295     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.12     |\n",
      "|    n_updates        | 1798     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 76       |\n",
      "|    ep_rew_mean      | -197     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 5053     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.01     |\n",
      "|    n_updates        | 1874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 75       |\n",
      "|    ep_rew_mean      | -182     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 5078     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7896     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.18     |\n",
      "|    n_updates        | 1948     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 74.5     |\n",
      "|    ep_rew_mean      | -173     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 5099     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8147     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.985    |\n",
      "|    n_updates        | 2011     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 70.6     |\n",
      "|    ep_rew_mean      | -162     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 5124     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8451     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.73     |\n",
      "|    n_updates        | 2087     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 70.4     |\n",
      "|    ep_rew_mean      | -152     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 5149     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8711     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.645    |\n",
      "|    n_updates        | 2152     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 70.4     |\n",
      "|    ep_rew_mean      | -142     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 5163     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8981     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.81     |\n",
      "|    n_updates        | 2220     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 70.3     |\n",
      "|    ep_rew_mean      | -138     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 5182     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9282     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 2295     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 69.7     |\n",
      "|    ep_rew_mean      | -135     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 5185     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9576     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.77     |\n",
      "|    n_updates        | 2368     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 69.3     |\n",
      "|    ep_rew_mean      | -137     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 5112     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9825     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.29     |\n",
      "|    n_updates        | 2431     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.7     |\n",
      "|    ep_rew_mean      | -137     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 5066     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 10103    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.4      |\n",
      "|    n_updates        | 2500     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.9     |\n",
      "|    ep_rew_mean      | -138     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 5036     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 10393    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 2573     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 69.2     |\n",
      "|    ep_rew_mean      | -139     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 5023     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 10690    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.771    |\n",
      "|    n_updates        | 2647     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.6     |\n",
      "|    ep_rew_mean      | -138     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 5012     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 10935    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.95     |\n",
      "|    n_updates        | 2708     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.8     |\n",
      "|    ep_rew_mean      | -140     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 5007     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 11209    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.396    |\n",
      "|    n_updates        | 2777     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.3     |\n",
      "|    ep_rew_mean      | -139     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 5003     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 11469    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.676    |\n",
      "|    n_updates        | 2842     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 69.8     |\n",
      "|    ep_rew_mean      | -139     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 5010     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 11853    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.469    |\n",
      "|    n_updates        | 2938     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 81.6     |\n",
      "|    ep_rew_mean      | -150     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 4955     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13289    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.487    |\n",
      "|    n_updates        | 3297     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x105762bd0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 98
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:38:43.816714Z",
     "start_time": "2025-04-03T13:38:43.090500Z"
    }
   },
   "source": [
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent, \n",
    "                                                                env_render, \n",
    "                                                                n_eval_episodes=10,\n",
    "                                                               render = True)\n",
    "print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[101], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m mean_reward, std_reward \u001B[38;5;241m=\u001B[39m sb3\u001B[38;5;241m.\u001B[39mcommon\u001B[38;5;241m.\u001B[39mevaluation\u001B[38;5;241m.\u001B[39mevaluate_policy(agent, \n\u001B[1;32m      2\u001B[0m                                                                 env_render, \n\u001B[1;32m      3\u001B[0m                                                                 n_eval_episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[1;32m      4\u001B[0m                                                                render \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMean Reward: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m +/- \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(mean_reward, std_reward))\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AI/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:94\u001B[0m, in \u001B[0;36mevaluate_policy\u001B[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m (episode_counts \u001B[38;5;241m<\u001B[39m episode_count_targets)\u001B[38;5;241m.\u001B[39many():\n\u001B[1;32m     88\u001B[0m     actions, states \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(\n\u001B[1;32m     89\u001B[0m         observations,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m     90\u001B[0m         state\u001B[38;5;241m=\u001B[39mstates,\n\u001B[1;32m     91\u001B[0m         episode_start\u001B[38;5;241m=\u001B[39mepisode_starts,\n\u001B[1;32m     92\u001B[0m         deterministic\u001B[38;5;241m=\u001B[39mdeterministic,\n\u001B[1;32m     93\u001B[0m     )\n\u001B[0;32m---> 94\u001B[0m     new_observations, rewards, dones, infos \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(actions)\n\u001B[1;32m     95\u001B[0m     current_rewards \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m rewards\n\u001B[1;32m     96\u001B[0m     current_lengths \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AI/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001B[0m, in \u001B[0;36mVecEnv.step\u001B[0;34m(self, actions)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;124;03mStep the environments with the given action\u001B[39;00m\n\u001B[1;32m    217\u001B[0m \n\u001B[1;32m    218\u001B[0m \u001B[38;5;124;03m:param actions: the action\u001B[39;00m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;124;03m:return: observation, reward, done, information\u001B[39;00m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_async(actions)\n\u001B[0;32m--> 222\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_wait()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AI/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001B[0m, in \u001B[0;36mDummyVecEnv.step_wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvStepReturn:\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# Avoid circular imports\u001B[39;00m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs):\n\u001B[0;32m---> 59\u001B[0m         obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_rews[env_idx], terminated, truncated, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_infos[env_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvs[env_idx]\u001B[38;5;241m.\u001B[39mstep(  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[1;32m     60\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactions[env_idx]\n\u001B[1;32m     61\u001B[0m         )\n\u001B[1;32m     62\u001B[0m         \u001B[38;5;66;03m# convert to SB3 VecEnv api\u001B[39;00m\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_dones[env_idx] \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AI/lib/python3.12/site-packages/gymnasium/wrappers/time_limit.py:57\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     47\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     55\u001B[0m \n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 57\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AI/lib/python3.12/site-packages/gymnasium/wrappers/time_limit.py:57\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     47\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     55\u001B[0m \n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 57\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AI/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AI/lib/python3.12/site-packages/gymnasium/wrappers/env_checker.py:49\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AI/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:675\u001B[0m, in \u001B[0;36mLunarLander.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    672\u001B[0m     reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m100\u001B[39m\n\u001B[1;32m    674\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 675\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender()\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(state, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32), reward, terminated, \u001B[38;5;28;01mFalse\u001B[39;00m, {}\n",
      "File \u001B[0;32m/opt/anaconda3/envs/AI/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:787\u001B[0m, in \u001B[0;36mLunarLander.render\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    785\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscreen\u001B[38;5;241m.\u001B[39mblit(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msurf, (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m))\n\u001B[1;32m    786\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n\u001B[0;32m--> 787\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclock\u001B[38;5;241m.\u001B[39mtick(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrender_fps\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    788\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mflip()\n\u001B[1;32m    789\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the agent in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env_render.reset()\n",
    "\n",
    "terminate = False\n",
    "truncate = False\n",
    "while not (terminate or truncate):\n",
    "    \n",
    "    action = env_render.action_space.sample()\n",
    "    obs, reward, terminate, truncate, _ = env_render.step(action)\n",
    "\n",
    "    env_render.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.2083px",
    "left": "801.167px",
    "top": "67.1354px",
    "width": "158.833px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
