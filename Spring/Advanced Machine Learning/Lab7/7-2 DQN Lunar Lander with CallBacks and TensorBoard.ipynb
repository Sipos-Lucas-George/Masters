{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590 - Advanced Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN for Lunar Lander Using Callbacks & TensorBoard\n",
    "Uses a Deep Q Network to train a neural network based player for the Lunar Lander environment from Gymnasium (https://gymnasium.farama.org/environments/box2d/lunar_lander/). This uses a vector-based state representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Google colab you need to isntall packages  - comment out lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt install swig cmake ffmpeg\n",
    "#!apt-get install -y xvfb x11-utils\n",
    "#!python -m pip install 'git+https://github.com/DLR-RM/stable-baselines3@feat/gymnasium-support#egg=stable-baselines3[extra]' \n",
    "#!pip install pyglet box2d box2d-kengz\n",
    "#!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Google colab comment out this cell to make a virtual rendering canvas so render calls work (we still wont; see display!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyvirtualdisplay\n",
    "#\n",
    "#_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "#                                    size=(1400, 900))\n",
    "#_ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 22:03:40.017557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import stable_baselines3 as sb3\n",
    "\n",
    "import pandas as pd # For data frames and data frame manipulation\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np # For general  numeric operations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Lunar Lander Environment with a TimeLimit wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = gym.make('LunarLander-v2')\n",
    "env_train = gym.wrappers.TimeLimit(env_train, \n",
    "                                   max_episode_steps=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train an Agent With an Evaluation Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple DQN agent using stable-baselines3. LunarLander uses a state vector representation so a simple MLP can drive this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "agent = sb3.DQN('MlpPolicy', \n",
    "                env_train, \n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an evaluation callback so that an evaluation is performed every 5000 time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make('LunarLander-v2', render_mode = 'human') # We use a separate evaluation env in case any wrappers have been used\n",
    "eval_callback = sb3.common.callbacks.EvalCallback(eval_env, \n",
    "                                                  best_model_save_path='./logs_lunarlander/',\n",
    "                                                  log_path='./logs_lunarlander/', \n",
    "                                                  eval_freq=100,\n",
    "                                                  render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent with the callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmacnamee/opt/anaconda3/envs/COMP47590_2024/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "2025-03-27 22:03:57.786 python3.12[7164:231649] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-27 22:03:57.786 python3.12[7164:231649] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=100, episode_reward=-949.59 +/- 252.08\n",
      "Episode length: 144.60 +/- 42.38\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 145      |\n",
      "|    mean_reward      | -950     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200, episode_reward=-1093.71 +/- 96.54\n",
      "Episode length: 162.80 +/- 23.82\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 163       |\n",
      "|    mean_reward      | -1.09e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 200       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.16      |\n",
      "|    n_updates        | 24        |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=300, episode_reward=-903.51 +/- 111.59\n",
      "Episode length: 154.60 +/- 34.90\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 155      |\n",
      "|    mean_reward      | -904     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.79     |\n",
      "|    n_updates        | 49       |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400, episode_reward=-684.91 +/- 60.29\n",
      "Episode length: 95.00 +/- 17.92\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 95       |\n",
      "|    mean_reward      | -685     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 400      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.06     |\n",
      "|    n_updates        | 74       |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=500, episode_reward=-775.15 +/- 214.06\n",
      "Episode length: 106.00 +/- 7.07\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 106      |\n",
      "|    mean_reward      | -775     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 500      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.7      |\n",
      "|    n_updates        | 99       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x10a098740>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.learn(total_timesteps=500, \n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the agent in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmacnamee/opt/anaconda3/envs/COMP47590_2024/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -688.7739686 +/- 90.26195957897468\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent, \n",
    "                                                                agent.get_env(), \n",
    "                                                                n_eval_episodes=10,\n",
    "                                                               render = True)\n",
    "print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine EvalCallback Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EvalCallback saves evaluation information as a numpy array which we can load for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_log = np.load('./logs_lunarlander/evaluations.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored as a series of *files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['timesteps', 'results', 'ep_lengths']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_log.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view each of these - note the multidimensional structure of results and ep_lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100, 200, 300, 400, 500])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_log['timesteps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1178.73304637, -1147.16844976,  -754.50844455, -1117.76109145,\n",
       "         -549.78781202],\n",
       "       [-1148.24115335, -1010.68573546, -1245.00699577,  -976.1866165 ,\n",
       "        -1088.44205707],\n",
       "       [ -714.63214807, -1052.67549746,  -961.07078769,  -873.74235133,\n",
       "         -915.45021689],\n",
       "       [ -755.8810806 ,  -695.56230264,  -597.06297755,  -636.59479553,\n",
       "         -739.45879098],\n",
       "       [ -681.68188085, -1175.11330764,  -554.25789191,  -669.35110384,\n",
       "         -795.32795338]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_log['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[202, 167,  99, 163,  92],\n",
       "       [190, 128, 182, 172, 142],\n",
       "       [106, 190, 176, 182, 119],\n",
       "       [103, 126,  79,  78,  89],\n",
       "       [ 96, 116, 101, 106, 111]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_log['ep_lengths']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert this into a pandas dataframe for easy analysis, plotting etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timesteps</th>\n",
       "      <th>results</th>\n",
       "      <th>ep_lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>-949.591769</td>\n",
       "      <td>144.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200.0</td>\n",
       "      <td>-1093.712512</td>\n",
       "      <td>162.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300.0</td>\n",
       "      <td>-903.514200</td>\n",
       "      <td>154.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400.0</td>\n",
       "      <td>-684.911989</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500.0</td>\n",
       "      <td>-775.146428</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timesteps      results  ep_lengths\n",
       "0      100.0  -949.591769       144.6\n",
       "1      200.0 -1093.712512       162.8\n",
       "2      300.0  -903.514200       154.6\n",
       "3      400.0  -684.911989        95.0\n",
       "4      500.0  -775.146428       106.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_log_df = pd.DataFrame({item: [np.mean(ep) for ep in evaluation_log[item]] for item in evaluation_log.files})\n",
    "display(evaluation_log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bz/lbfg_nj54wd40pqchhwx6ys80000gp/T/ipykernel_7164/3682008506.py:3: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(evaluation_log_df['timesteps'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = evaluation_log_df.loc[0:len(evaluation_log_df), 'results'].plot(color = 'lightgray', xlim = [-5, len(evaluation_log_df)], figsize = (10,5))\n",
    "evaluation_log_df['results'].rolling(5).mean().plot(color = 'black', xlim = [-5, len(evaluation_log_df)])\n",
    "ax.set_xticklabels(evaluation_log_df['timesteps'])\n",
    "ax.set_xlabel(\"Eval Episode\")\n",
    "plt.ylabel(\"Rolling Mean Cumulative Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load the best model saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent = sb3.dqn.DQN.load(\"./logs_lunarlander/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the agent in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmacnamee/opt/anaconda3/envs/COMP47590_2024/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -655.9170818346553 +/- 111.02318047279253\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(best_agent, \n",
    "                                                                eval_env, \n",
    "                                                                n_eval_episodes=10,\n",
    "                                                               render = True)\n",
    "print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **TensorBoard** to monitor the learning process in real time and historically. Simply set up a log location when we create the DQN agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "tb_log = './log_tb_lunarlander/'\n",
    "agent = sb3.DQN('MlpPolicy', \n",
    "                env_train, \n",
    "                verbose=1, \n",
    "                tensorboard_log=tb_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make another evaluation callback with a much longer wait between steps and no rendering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make('LunarLander-v2', render_mode = 'human') # We use a separate evaluation env in case any wrappers have been used\n",
    "eval_callback = sb3.common.callbacks.EvalCallback(eval_env, \n",
    "                                                  best_model_save_path='./logs_lunarlander/',\n",
    "                                                  log_path='./logs_lunarlander/', \n",
    "                                                  eval_freq=5000,\n",
    "                                                  render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./log_tb_lunarlander/Basic DQN Network_8\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -167     |\n",
      "|    exploration_rate | 0.996    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 827      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 421      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.32     |\n",
      "|    n_updates        | 80       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.8     |\n",
      "|    ep_rew_mean      | -166     |\n",
      "|    exploration_rate | 0.992    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 729      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 798      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.23     |\n",
      "|    n_updates        | 174      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99       |\n",
      "|    ep_rew_mean      | -155     |\n",
      "|    exploration_rate | 0.989    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 755      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 1188     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.69     |\n",
      "|    n_updates        | 271      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 96.3     |\n",
      "|    ep_rew_mean      | -161     |\n",
      "|    exploration_rate | 0.985    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 770      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 1541     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.954    |\n",
      "|    n_updates        | 360      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.2     |\n",
      "|    ep_rew_mean      | -173     |\n",
      "|    exploration_rate | 0.982    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 783      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 1883     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.62     |\n",
      "|    n_updates        | 445      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.2     |\n",
      "|    ep_rew_mean      | -170     |\n",
      "|    exploration_rate | 0.979    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 770      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 2236     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.63     |\n",
      "|    n_updates        | 533      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.7     |\n",
      "|    ep_rew_mean      | -166     |\n",
      "|    exploration_rate | 0.975    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 786      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 2651     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.968    |\n",
      "|    n_updates        | 637      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.2     |\n",
      "|    ep_rew_mean      | -166     |\n",
      "|    exploration_rate | 0.971    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 791      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 3048     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 736      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.2     |\n",
      "|    ep_rew_mean      | -156     |\n",
      "|    exploration_rate | 0.967    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 786      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 3429     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.79     |\n",
      "|    n_updates        | 832      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.7     |\n",
      "|    ep_rew_mean      | -156     |\n",
      "|    exploration_rate | 0.964    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 758      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 3786     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.55     |\n",
      "|    n_updates        | 921      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.7     |\n",
      "|    ep_rew_mean      | -158     |\n",
      "|    exploration_rate | 0.96     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 763      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 4168     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.795    |\n",
      "|    n_updates        | 1016     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94       |\n",
      "|    ep_rew_mean      | -160     |\n",
      "|    exploration_rate | 0.957    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 752      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 4514     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.6      |\n",
      "|    n_updates        | 1103     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.8     |\n",
      "|    ep_rew_mean      | -164     |\n",
      "|    exploration_rate | 0.953    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 758      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 4929     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.717    |\n",
      "|    n_updates        | 1207     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-799.49 +/- 136.41\n",
      "Episode length: 130.60 +/- 28.61\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 131      |\n",
      "|    mean_reward      | -799     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.953    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.655    |\n",
      "|    n_updates        | 1224     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.2     |\n",
      "|    ep_rew_mean      | -166     |\n",
      "|    exploration_rate | 0.95     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 5220     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.668    |\n",
      "|    n_updates        | 1279     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.9     |\n",
      "|    ep_rew_mean      | -167     |\n",
      "|    exploration_rate | 0.946    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 259      |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 5632     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.36     |\n",
      "|    n_updates        | 1382     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.6     |\n",
      "|    ep_rew_mean      | -169     |\n",
      "|    exploration_rate | 0.943    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 266      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 5993     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.72     |\n",
      "|    n_updates        | 1473     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.2     |\n",
      "|    ep_rew_mean      | -167     |\n",
      "|    exploration_rate | 0.939    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 276      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 6408     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.52     |\n",
      "|    n_updates        | 1576     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.1     |\n",
      "|    ep_rew_mean      | -167     |\n",
      "|    exploration_rate | 0.935    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 288      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 6850     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 1687     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.9     |\n",
      "|    ep_rew_mean      | -168     |\n",
      "|    exploration_rate | 0.931    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 300      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7290     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.28     |\n",
      "|    n_updates        | 1797     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 96.6     |\n",
      "|    ep_rew_mean      | -170     |\n",
      "|    exploration_rate | 0.927    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7727     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.669    |\n",
      "|    n_updates        | 1906     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 96.7     |\n",
      "|    ep_rew_mean      | -171     |\n",
      "|    exploration_rate | 0.923    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 319      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8125     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.17     |\n",
      "|    n_updates        | 2006     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 97.4     |\n",
      "|    ep_rew_mean      | -170     |\n",
      "|    exploration_rate | 0.919    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8567     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.932    |\n",
      "|    n_updates        | 2116     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 97.2     |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.915    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 336      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8942     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.05     |\n",
      "|    n_updates        | 2210     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 97.2     |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.911    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 345      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9333     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.72     |\n",
      "|    n_updates        | 2308     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 97.9     |\n",
      "|    ep_rew_mean      | -174     |\n",
      "|    exploration_rate | 0.907    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 355      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9789     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.77     |\n",
      "|    n_updates        | 2422     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-362.57 +/- 53.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -363     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.84     |\n",
      "|    n_updates        | 2474     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.2     |\n",
      "|    ep_rew_mean      | -175     |\n",
      "|    exploration_rate | 0.903    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 78       |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 10243    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.03     |\n",
      "|    n_updates        | 2535     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.9     |\n",
      "|    ep_rew_mean      | -174     |\n",
      "|    exploration_rate | 0.898    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 81       |\n",
      "|    time_elapsed     | 131      |\n",
      "|    total_timesteps  | 10685    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.742    |\n",
      "|    n_updates        | 2646     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.7     |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.895    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 83       |\n",
      "|    time_elapsed     | 131      |\n",
      "|    total_timesteps  | 11056    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.96     |\n",
      "|    n_updates        | 2738     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.6     |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.891    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 86       |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total_timesteps  | 11502    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.69     |\n",
      "|    n_updates        | 2850     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -171     |\n",
      "|    exploration_rate | 0.887    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 89       |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total_timesteps  | 11894    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.94     |\n",
      "|    n_updates        | 2948     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -173     |\n",
      "|    exploration_rate | 0.883    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 92       |\n",
      "|    time_elapsed     | 133      |\n",
      "|    total_timesteps  | 12285    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.809    |\n",
      "|    n_updates        | 3046     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -175     |\n",
      "|    exploration_rate | 0.88     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 94       |\n",
      "|    time_elapsed     | 133      |\n",
      "|    total_timesteps  | 12677    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.367    |\n",
      "|    n_updates        | 3144     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | -174     |\n",
      "|    exploration_rate | 0.875    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 97       |\n",
      "|    time_elapsed     | 134      |\n",
      "|    total_timesteps  | 13117    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.808    |\n",
      "|    n_updates        | 3254     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.871    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 100      |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total_timesteps  | 13612    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.47     |\n",
      "|    n_updates        | 3377     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.867    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 103      |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total_timesteps  | 14052    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.92     |\n",
      "|    n_updates        | 3487     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.862    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 106      |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 14495    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.79     |\n",
      "|    n_updates        | 3598     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -177     |\n",
      "|    exploration_rate | 0.858    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 108      |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 14918    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.845    |\n",
      "|    n_updates        | 3704     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-235.61 +/- 8.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -236     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.858    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.624    |\n",
      "|    n_updates        | 3724     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -175     |\n",
      "|    exploration_rate | 0.854    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 63       |\n",
      "|    time_elapsed     | 240      |\n",
      "|    total_timesteps  | 15352    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.496    |\n",
      "|    n_updates        | 3812     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -173     |\n",
      "|    exploration_rate | 0.85     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 241      |\n",
      "|    total_timesteps  | 15767    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 12.7     |\n",
      "|    n_updates        | 3916     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -173     |\n",
      "|    exploration_rate | 0.846    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 241      |\n",
      "|    total_timesteps  | 16163    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.26     |\n",
      "|    n_updates        | 4015     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -173     |\n",
      "|    exploration_rate | 0.842    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 242      |\n",
      "|    total_timesteps  | 16623    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.375    |\n",
      "|    n_updates        | 4130     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -170     |\n",
      "|    exploration_rate | 0.839    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 69       |\n",
      "|    time_elapsed     | 243      |\n",
      "|    total_timesteps  | 16977    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.68     |\n",
      "|    n_updates        | 4219     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -169     |\n",
      "|    exploration_rate | 0.836    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 71       |\n",
      "|    time_elapsed     | 243      |\n",
      "|    total_timesteps  | 17314    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.564    |\n",
      "|    n_updates        | 4303     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -169     |\n",
      "|    exploration_rate | 0.831    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 72       |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total_timesteps  | 17769    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.667    |\n",
      "|    n_updates        | 4417     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -169     |\n",
      "|    exploration_rate | 0.827    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 74       |\n",
      "|    time_elapsed     | 245      |\n",
      "|    total_timesteps  | 18187    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.416    |\n",
      "|    n_updates        | 4521     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -163     |\n",
      "|    exploration_rate | 0.823    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 75       |\n",
      "|    time_elapsed     | 245      |\n",
      "|    total_timesteps  | 18636    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.51     |\n",
      "|    n_updates        | 4633     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -163     |\n",
      "|    exploration_rate | 0.819    |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 77       |\n",
      "|    time_elapsed     | 246      |\n",
      "|    total_timesteps  | 19034    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.366    |\n",
      "|    n_updates        | 4733     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -154     |\n",
      "|    exploration_rate | 0.816    |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 78       |\n",
      "|    time_elapsed     | 247      |\n",
      "|    total_timesteps  | 19404    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.876    |\n",
      "|    n_updates        | 4825     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -153     |\n",
      "|    exploration_rate | 0.812    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 79       |\n",
      "|    time_elapsed     | 247      |\n",
      "|    total_timesteps  | 19821    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.76     |\n",
      "|    n_updates        | 4930     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-137.78 +/- 15.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -138     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.435    |\n",
      "|    n_updates        | 4974     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -157     |\n",
      "|    exploration_rate | 0.808    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 57       |\n",
      "|    time_elapsed     | 351      |\n",
      "|    total_timesteps  | 20195    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 5023     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | -154     |\n",
      "|    exploration_rate | 0.805    |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 58       |\n",
      "|    time_elapsed     | 351      |\n",
      "|    total_timesteps  | 20573    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.765    |\n",
      "|    n_updates        | 5118     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | -156     |\n",
      "|    exploration_rate | 0.8      |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 352      |\n",
      "|    total_timesteps  | 21013    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.11     |\n",
      "|    n_updates        | 5228     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -153     |\n",
      "|    exploration_rate | 0.796    |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 353      |\n",
      "|    total_timesteps  | 21497    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.952    |\n",
      "|    n_updates        | 5349     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -150     |\n",
      "|    exploration_rate | 0.791    |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 353      |\n",
      "|    total_timesteps  | 22034    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.18     |\n",
      "|    n_updates        | 5483     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -153     |\n",
      "|    exploration_rate | 0.787    |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 63       |\n",
      "|    time_elapsed     | 354      |\n",
      "|    total_timesteps  | 22430    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.5      |\n",
      "|    n_updates        | 5582     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -149     |\n",
      "|    exploration_rate | 0.782    |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 64       |\n",
      "|    time_elapsed     | 355      |\n",
      "|    total_timesteps  | 22904    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.597    |\n",
      "|    n_updates        | 5700     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 107      |\n",
      "|    ep_rew_mean      | -150     |\n",
      "|    exploration_rate | 0.778    |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 355      |\n",
      "|    total_timesteps  | 23400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.539    |\n",
      "|    n_updates        | 5824     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 107      |\n",
      "|    ep_rew_mean      | -147     |\n",
      "|    exploration_rate | 0.774    |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 356      |\n",
      "|    total_timesteps  | 23821    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.71     |\n",
      "|    n_updates        | 5930     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -148     |\n",
      "|    exploration_rate | 0.77     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 356      |\n",
      "|    total_timesteps  | 24195    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.9      |\n",
      "|    n_updates        | 6023     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -148     |\n",
      "|    exploration_rate | 0.766    |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 357      |\n",
      "|    total_timesteps  | 24671    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.88     |\n",
      "|    n_updates        | 6142     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-126.39 +/- 15.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -126     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.763    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.744    |\n",
      "|    n_updates        | 6224     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -148     |\n",
      "|    exploration_rate | 0.762    |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 461      |\n",
      "|    total_timesteps  | 25076    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.5      |\n",
      "|    n_updates        | 6243     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -145     |\n",
      "|    exploration_rate | 0.757    |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 461      |\n",
      "|    total_timesteps  | 25560    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.77     |\n",
      "|    n_updates        | 6364     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 107      |\n",
      "|    ep_rew_mean      | -142     |\n",
      "|    exploration_rate | 0.753    |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 56       |\n",
      "|    time_elapsed     | 462      |\n",
      "|    total_timesteps  | 26019    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.64     |\n",
      "|    n_updates        | 6479     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 107      |\n",
      "|    ep_rew_mean      | -142     |\n",
      "|    exploration_rate | 0.749    |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 57       |\n",
      "|    time_elapsed     | 463      |\n",
      "|    total_timesteps  | 26427    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 6581     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 107      |\n",
      "|    ep_rew_mean      | -141     |\n",
      "|    exploration_rate | 0.744    |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 58       |\n",
      "|    time_elapsed     | 463      |\n",
      "|    total_timesteps  | 26906    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.662    |\n",
      "|    n_updates        | 6701     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 108      |\n",
      "|    ep_rew_mean      | -139     |\n",
      "|    exploration_rate | 0.74     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 58       |\n",
      "|    time_elapsed     | 464      |\n",
      "|    total_timesteps  | 27391    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.643    |\n",
      "|    n_updates        | 6822     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 109      |\n",
      "|    ep_rew_mean      | -141     |\n",
      "|    exploration_rate | 0.735    |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 465      |\n",
      "|    total_timesteps  | 27891    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.623    |\n",
      "|    n_updates        | 6947     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 110      |\n",
      "|    ep_rew_mean      | -140     |\n",
      "|    exploration_rate | 0.731    |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 465      |\n",
      "|    total_timesteps  | 28290    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.46     |\n",
      "|    n_updates        | 7047     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 110      |\n",
      "|    ep_rew_mean      | -135     |\n",
      "|    exploration_rate | 0.726    |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 466      |\n",
      "|    total_timesteps  | 28795    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 7173     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 110      |\n",
      "|    ep_rew_mean      | -131     |\n",
      "|    exploration_rate | 0.723    |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 466      |\n",
      "|    total_timesteps  | 29142    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.98     |\n",
      "|    n_updates        | 7260     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 109      |\n",
      "|    ep_rew_mean      | -131     |\n",
      "|    exploration_rate | 0.719    |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 63       |\n",
      "|    time_elapsed     | 467      |\n",
      "|    total_timesteps  | 29580    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.53     |\n",
      "|    n_updates        | 7369     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-122.28 +/- 17.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -122     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.715    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.07     |\n",
      "|    n_updates        | 7474     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 111      |\n",
      "|    ep_rew_mean      | -129     |\n",
      "|    exploration_rate | 0.714    |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 571      |\n",
      "|    total_timesteps  | 30108    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.883    |\n",
      "|    n_updates        | 7501     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | -129     |\n",
      "|    exploration_rate | 0.709    |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 572      |\n",
      "|    total_timesteps  | 30625    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.812    |\n",
      "|    n_updates        | 7631     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 113      |\n",
      "|    ep_rew_mean      | -129     |\n",
      "|    exploration_rate | 0.705    |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 573      |\n",
      "|    total_timesteps  | 31079    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.96     |\n",
      "|    n_updates        | 7744     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 113      |\n",
      "|    ep_rew_mean      | -125     |\n",
      "|    exploration_rate | 0.701    |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 574      |\n",
      "|    total_timesteps  | 31493    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.514    |\n",
      "|    n_updates        | 7848     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 114      |\n",
      "|    ep_rew_mean      | -124     |\n",
      "|    exploration_rate | 0.696    |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 576      |\n",
      "|    total_timesteps  | 32023    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.835    |\n",
      "|    n_updates        | 7980     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 116      |\n",
      "|    ep_rew_mean      | -118     |\n",
      "|    exploration_rate | 0.69     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 56       |\n",
      "|    time_elapsed     | 577      |\n",
      "|    total_timesteps  | 32647    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.11     |\n",
      "|    n_updates        | 8136     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 116      |\n",
      "|    ep_rew_mean      | -117     |\n",
      "|    exploration_rate | 0.685    |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 57       |\n",
      "|    time_elapsed     | 580      |\n",
      "|    total_timesteps  | 33129    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.81     |\n",
      "|    n_updates        | 8257     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 116      |\n",
      "|    ep_rew_mean      | -115     |\n",
      "|    exploration_rate | 0.681    |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 57       |\n",
      "|    time_elapsed     | 582      |\n",
      "|    total_timesteps  | 33625    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.457    |\n",
      "|    n_updates        | 8381     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 118      |\n",
      "|    ep_rew_mean      | -114     |\n",
      "|    exploration_rate | 0.675    |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 58       |\n",
      "|    time_elapsed     | 583      |\n",
      "|    total_timesteps  | 34193    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.07     |\n",
      "|    n_updates        | 8523     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 119      |\n",
      "|    ep_rew_mean      | -112     |\n",
      "|    exploration_rate | 0.669    |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 584      |\n",
      "|    total_timesteps  | 34801    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.93     |\n",
      "|    n_updates        | 8675     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-105.87 +/- 23.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -106     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.668    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 35000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.462    |\n",
      "|    n_updates        | 8724     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 119      |\n",
      "|    ep_rew_mean      | -109     |\n",
      "|    exploration_rate | 0.664    |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 688      |\n",
      "|    total_timesteps  | 35339    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.861    |\n",
      "|    n_updates        | 8809     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 121      |\n",
      "|    ep_rew_mean      | -110     |\n",
      "|    exploration_rate | 0.659    |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 688      |\n",
      "|    total_timesteps  | 35942    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.504    |\n",
      "|    n_updates        | 8960     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 121      |\n",
      "|    ep_rew_mean      | -108     |\n",
      "|    exploration_rate | 0.655    |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 689      |\n",
      "|    total_timesteps  | 36314    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.51     |\n",
      "|    n_updates        | 9053     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 122      |\n",
      "|    ep_rew_mean      | -104     |\n",
      "|    exploration_rate | 0.65     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 690      |\n",
      "|    total_timesteps  | 36836    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.82     |\n",
      "|    n_updates        | 9183     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 123      |\n",
      "|    ep_rew_mean      | -103     |\n",
      "|    exploration_rate | 0.645    |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 690      |\n",
      "|    total_timesteps  | 37357    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.61     |\n",
      "|    n_updates        | 9314     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 123      |\n",
      "|    ep_rew_mean      | -103     |\n",
      "|    exploration_rate | 0.641    |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 691      |\n",
      "|    total_timesteps  | 37821    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.839    |\n",
      "|    n_updates        | 9430     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 123      |\n",
      "|    ep_rew_mean      | -103     |\n",
      "|    exploration_rate | 0.636    |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 692      |\n",
      "|    total_timesteps  | 38354    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.903    |\n",
      "|    n_updates        | 9563     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 126      |\n",
      "|    ep_rew_mean      | -103     |\n",
      "|    exploration_rate | 0.629    |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 56       |\n",
      "|    time_elapsed     | 692      |\n",
      "|    total_timesteps  | 39060    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.45     |\n",
      "|    n_updates        | 9739     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 128      |\n",
      "|    ep_rew_mean      | -99.5    |\n",
      "|    exploration_rate | 0.623    |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 57       |\n",
      "|    time_elapsed     | 693      |\n",
      "|    total_timesteps  | 39692    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.457    |\n",
      "|    n_updates        | 9897     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-134.93 +/- 23.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -135     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.834    |\n",
      "|    n_updates        | 9974     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 128      |\n",
      "|    ep_rew_mean      | -97.3    |\n",
      "|    exploration_rate | 0.618    |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 797      |\n",
      "|    total_timesteps  | 40208    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.36     |\n",
      "|    n_updates        | 10026    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 130      |\n",
      "|    ep_rew_mean      | -95.5    |\n",
      "|    exploration_rate | 0.611    |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 798      |\n",
      "|    total_timesteps  | 40926    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.11     |\n",
      "|    n_updates        | 10206    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 140      |\n",
      "|    ep_rew_mean      | -96.2    |\n",
      "|    exploration_rate | 0.598    |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 801      |\n",
      "|    total_timesteps  | 42292    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.424    |\n",
      "|    n_updates        | 10547    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 141      |\n",
      "|    ep_rew_mean      | -97.2    |\n",
      "|    exploration_rate | 0.593    |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 802      |\n",
      "|    total_timesteps  | 42853    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.669    |\n",
      "|    n_updates        | 10688    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 143      |\n",
      "|    ep_rew_mean      | -98.3    |\n",
      "|    exploration_rate | 0.587    |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 802      |\n",
      "|    total_timesteps  | 43463    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.01     |\n",
      "|    n_updates        | 10840    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 145      |\n",
      "|    ep_rew_mean      | -99.3    |\n",
      "|    exploration_rate | 0.581    |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 803      |\n",
      "|    total_timesteps  | 44092    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.58     |\n",
      "|    n_updates        | 10997    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 145      |\n",
      "|    ep_rew_mean      | -99.7    |\n",
      "|    exploration_rate | 0.576    |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 804      |\n",
      "|    total_timesteps  | 44633    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.688    |\n",
      "|    n_updates        | 11133    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-109.74 +/- 19.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -110     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.573    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 45000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.04     |\n",
      "|    n_updates        | 11224    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 146      |\n",
      "|    ep_rew_mean      | -99.9    |\n",
      "|    exploration_rate | 0.57     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 49       |\n",
      "|    time_elapsed     | 908      |\n",
      "|    total_timesteps  | 45241    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.68     |\n",
      "|    n_updates        | 11285    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 147      |\n",
      "|    ep_rew_mean      | -95.8    |\n",
      "|    exploration_rate | 0.565    |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 908      |\n",
      "|    total_timesteps  | 45739    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.392    |\n",
      "|    n_updates        | 11409    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 148      |\n",
      "|    ep_rew_mean      | -98.7    |\n",
      "|    exploration_rate | 0.56     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 909      |\n",
      "|    total_timesteps  | 46308    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.577    |\n",
      "|    n_updates        | 11551    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 149      |\n",
      "|    ep_rew_mean      | -100     |\n",
      "|    exploration_rate | 0.554    |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 910      |\n",
      "|    total_timesteps  | 46941    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.668    |\n",
      "|    n_updates        | 11710    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 151      |\n",
      "|    ep_rew_mean      | -104     |\n",
      "|    exploration_rate | 0.546    |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 911      |\n",
      "|    total_timesteps  | 47747    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.517    |\n",
      "|    n_updates        | 11911    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 153      |\n",
      "|    ep_rew_mean      | -105     |\n",
      "|    exploration_rate | 0.54     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 912      |\n",
      "|    total_timesteps  | 48471    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.601    |\n",
      "|    n_updates        | 12092    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 154      |\n",
      "|    ep_rew_mean      | -106     |\n",
      "|    exploration_rate | 0.534    |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 913      |\n",
      "|    total_timesteps  | 49000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.546    |\n",
      "|    n_updates        | 12224    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 156      |\n",
      "|    ep_rew_mean      | -104     |\n",
      "|    exploration_rate | 0.527    |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 913      |\n",
      "|    total_timesteps  | 49781    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.692    |\n",
      "|    n_updates        | 12420    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-146.78 +/- 17.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -147     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.472    |\n",
      "|    n_updates        | 12474    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 158      |\n",
      "|    ep_rew_mean      | -106     |\n",
      "|    exploration_rate | 0.519    |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 49       |\n",
      "|    time_elapsed     | 1018     |\n",
      "|    total_timesteps  | 50593    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.27     |\n",
      "|    n_updates        | 12623    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 160      |\n",
      "|    ep_rew_mean      | -105     |\n",
      "|    exploration_rate | 0.512    |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 1019     |\n",
      "|    total_timesteps  | 51376    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.623    |\n",
      "|    n_updates        | 12818    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 163      |\n",
      "|    ep_rew_mean      | -104     |\n",
      "|    exploration_rate | 0.504    |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 1020     |\n",
      "|    total_timesteps  | 52195    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.695    |\n",
      "|    n_updates        | 13023    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 173      |\n",
      "|    ep_rew_mean      | -102     |\n",
      "|    exploration_rate | 0.491    |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 1022     |\n",
      "|    total_timesteps  | 53591    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4        |\n",
      "|    n_updates        | 13372    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 174      |\n",
      "|    ep_rew_mean      | -100     |\n",
      "|    exploration_rate | 0.485    |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 1022     |\n",
      "|    total_timesteps  | 54241    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.438    |\n",
      "|    n_updates        | 13535    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 175      |\n",
      "|    ep_rew_mean      | -96.7    |\n",
      "|    exploration_rate | 0.478    |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 1023     |\n",
      "|    total_timesteps  | 54905    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.623    |\n",
      "|    n_updates        | 13701    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 198      |\n",
      "|    ep_rew_mean      | -251     |\n",
      "|    exploration_rate | 0.359    |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 175      |\n",
      "|    time_elapsed     | 384      |\n",
      "|    total_timesteps  | 67457    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.705    |\n",
      "|    n_updates        | 4364     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-136.39 +/- 8.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -136     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.335    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.626    |\n",
      "|    n_updates        | 4999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 229      |\n",
      "|    ep_rew_mean      | -249     |\n",
      "|    exploration_rate | 0.325    |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 144      |\n",
      "|    time_elapsed     | 490      |\n",
      "|    total_timesteps  | 71056    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.707    |\n",
      "|    n_updates        | 5263     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 234      |\n",
      "|    ep_rew_mean      | -243     |\n",
      "|    exploration_rate | 0.317    |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 146      |\n",
      "|    time_elapsed     | 491      |\n",
      "|    total_timesteps  | 71895    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.47     |\n",
      "|    n_updates        | 5473     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-64.43 +/- 51.76\n",
      "Episode length: 860.20 +/- 279.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 860      |\n",
      "|    mean_reward      | -64.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.288    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 75000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.797    |\n",
      "|    n_updates        | 6249     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 270      |\n",
      "|    ep_rew_mean      | -235     |\n",
      "|    exploration_rate | 0.279    |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 130      |\n",
      "|    time_elapsed     | 583      |\n",
      "|    total_timesteps  | 75895    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.56     |\n",
      "|    n_updates        | 6473     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 294      |\n",
      "|    ep_rew_mean      | -232     |\n",
      "|    exploration_rate | 0.253    |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 586      |\n",
      "|    total_timesteps  | 78603    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.8      |\n",
      "|    n_updates        | 7150     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-110.44 +/- 20.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -110     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.7      |\n",
      "|    n_updates        | 7499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 309      |\n",
      "|    ep_rew_mean      | -226     |\n",
      "|    exploration_rate | 0.236    |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 116      |\n",
      "|    time_elapsed     | 691      |\n",
      "|    total_timesteps  | 80408    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.76     |\n",
      "|    n_updates        | 7601     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 333      |\n",
      "|    ep_rew_mean      | -219     |\n",
      "|    exploration_rate | 0.21     |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 119      |\n",
      "|    time_elapsed     | 693      |\n",
      "|    total_timesteps  | 83169    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.435    |\n",
      "|    n_updates        | 8292     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-93.44 +/- 12.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -93.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.193    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 85000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.38     |\n",
      "|    n_updates        | 8749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 353      |\n",
      "|    ep_rew_mean      | -211     |\n",
      "|    exploration_rate | 0.186    |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 107      |\n",
      "|    time_elapsed     | 800      |\n",
      "|    total_timesteps  | 85691    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.02     |\n",
      "|    n_updates        | 8922     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 373      |\n",
      "|    ep_rew_mean      | -192     |\n",
      "|    exploration_rate | 0.163    |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 109      |\n",
      "|    time_elapsed     | 802      |\n",
      "|    total_timesteps  | 88119    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.47     |\n",
      "|    n_updates        | 9529     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-117.45 +/- 21.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -117     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.145    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.771    |\n",
      "|    n_updates        | 9999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 409      |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.125    |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 101      |\n",
      "|    time_elapsed     | 910      |\n",
      "|    total_timesteps  | 92119    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.969    |\n",
      "|    n_updates        | 10529    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-266.86 +/- 159.25\n",
      "Episode length: 539.80 +/- 375.76\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 540      |\n",
      "|    mean_reward      | -267     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0975   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 95000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.11     |\n",
      "|    n_updates        | 11249    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 442      |\n",
      "|    ep_rew_mean      | -155     |\n",
      "|    exploration_rate | 0.0869   |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 99       |\n",
      "|    time_elapsed     | 969      |\n",
      "|    total_timesteps  | 96119    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.81     |\n",
      "|    n_updates        | 11529    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-208.03 +/- 150.66\n",
      "Episode length: 696.00 +/- 372.32\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 696      |\n",
      "|    mean_reward      | -208     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.55     |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 477      |\n",
      "|    ep_rew_mean      | -139     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 95       |\n",
      "|    time_elapsed     | 1045     |\n",
      "|    total_timesteps  | 100119   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.724    |\n",
      "|    n_updates        | 12529    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 509      |\n",
      "|    ep_rew_mean      | -119     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 99       |\n",
      "|    time_elapsed     | 1048     |\n",
      "|    total_timesteps  | 104119   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1        |\n",
      "|    n_updates        | 13529    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-137.08 +/- 220.86\n",
      "Episode length: 734.60 +/- 327.68\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 735      |\n",
      "|    mean_reward      | -137     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 105000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.467    |\n",
      "|    n_updates        | 13749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 534      |\n",
      "|    ep_rew_mean      | -111     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 95       |\n",
      "|    time_elapsed     | 1128     |\n",
      "|    total_timesteps  | 107374   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.81     |\n",
      "|    n_updates        | 14343    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-156.88 +/- 86.55\n",
      "Episode length: 852.20 +/- 295.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 852      |\n",
      "|    mean_reward      | -157     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 110000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.36     |\n",
      "|    n_updates        | 14999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 562      |\n",
      "|    ep_rew_mean      | -109     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 90       |\n",
      "|    time_elapsed     | 1221     |\n",
      "|    total_timesteps  | 110595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.826    |\n",
      "|    n_updates        | 15148    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 596      |\n",
      "|    ep_rew_mean      | -106     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 93       |\n",
      "|    time_elapsed     | 1225     |\n",
      "|    total_timesteps  | 114595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.511    |\n",
      "|    n_updates        | 16148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-116.38 +/- 13.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -116     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 115000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 16249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 625      |\n",
      "|    ep_rew_mean      | -104     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 88       |\n",
      "|    time_elapsed     | 1332     |\n",
      "|    total_timesteps  | 118595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.837    |\n",
      "|    n_updates        | 17148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-130.53 +/- 20.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -131     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 120000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.403    |\n",
      "|    n_updates        | 17499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 658      |\n",
      "|    ep_rew_mean      | -99.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 85       |\n",
      "|    time_elapsed     | 1440     |\n",
      "|    total_timesteps  | 122595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.475    |\n",
      "|    n_updates        | 18148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-134.51 +/- 24.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -135     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 125000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.341    |\n",
      "|    n_updates        | 18749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 689      |\n",
      "|    ep_rew_mean      | -97.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 81       |\n",
      "|    time_elapsed     | 1547     |\n",
      "|    total_timesteps  | 126595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.47     |\n",
      "|    n_updates        | 19148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-139.96 +/- 13.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -140     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 130000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.417    |\n",
      "|    n_updates        | 19999    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 720      |\n",
      "|    ep_rew_mean      | -97.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 78       |\n",
      "|    time_elapsed     | 1655     |\n",
      "|    total_timesteps  | 130595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.87     |\n",
      "|    n_updates        | 20148    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 749      |\n",
      "|    ep_rew_mean      | -97.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 81       |\n",
      "|    time_elapsed     | 1660     |\n",
      "|    total_timesteps  | 134595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.434    |\n",
      "|    n_updates        | 21148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-147.83 +/- 11.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -148     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 135000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.89     |\n",
      "|    n_updates        | 21249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 774      |\n",
      "|    ep_rew_mean      | -94.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 78       |\n",
      "|    time_elapsed     | 1767     |\n",
      "|    total_timesteps  | 138595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.1      |\n",
      "|    n_updates        | 22148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-81.47 +/- 73.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -81.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 140000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.757    |\n",
      "|    n_updates        | 22499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 805      |\n",
      "|    ep_rew_mean      | -92.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 76       |\n",
      "|    time_elapsed     | 1875     |\n",
      "|    total_timesteps  | 142595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.97     |\n",
      "|    n_updates        | 23148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-144.23 +/- 26.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -144     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 145000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 23749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 826      |\n",
      "|    ep_rew_mean      | -94      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 73       |\n",
      "|    time_elapsed     | 1983     |\n",
      "|    total_timesteps  | 146595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.932    |\n",
      "|    n_updates        | 24148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-153.99 +/- 9.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -154     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.59     |\n",
      "|    n_updates        | 24999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 856      |\n",
      "|    ep_rew_mean      | -95.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 72       |\n",
      "|    time_elapsed     | 2090     |\n",
      "|    total_timesteps  | 150595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.616    |\n",
      "|    n_updates        | 25148    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 871      |\n",
      "|    ep_rew_mean      | -93      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 73       |\n",
      "|    time_elapsed     | 2094     |\n",
      "|    total_timesteps  | 154595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.607    |\n",
      "|    n_updates        | 26148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-144.38 +/- 10.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -144     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 155000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.77     |\n",
      "|    n_updates        | 26249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 875      |\n",
      "|    ep_rew_mean      | -92.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 72       |\n",
      "|    time_elapsed     | 2202     |\n",
      "|    total_timesteps  | 158595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.59     |\n",
      "|    n_updates        | 27148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-128.18 +/- 49.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -128     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 160000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.487    |\n",
      "|    n_updates        | 27499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 907      |\n",
      "|    ep_rew_mean      | -92      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 70       |\n",
      "|    time_elapsed     | 2312     |\n",
      "|    total_timesteps  | 162595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.698    |\n",
      "|    n_updates        | 28148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-173.33 +/- 10.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -173     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 165000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.59     |\n",
      "|    n_updates        | 28749    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 907      |\n",
      "|    ep_rew_mean      | -95.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 2422     |\n",
      "|    total_timesteps  | 166595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.563    |\n",
      "|    n_updates        | 29148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-151.81 +/- 29.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -152     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 170000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.489    |\n",
      "|    n_updates        | 29999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 920      |\n",
      "|    ep_rew_mean      | -98.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 2535     |\n",
      "|    total_timesteps  | 170595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.736    |\n",
      "|    n_updates        | 30148    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 942      |\n",
      "|    ep_rew_mean      | -101     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 2539     |\n",
      "|    total_timesteps  | 174595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.17     |\n",
      "|    n_updates        | 31148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-149.11 +/- 22.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -149     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 175000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.504    |\n",
      "|    n_updates        | 31249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 954      |\n",
      "|    ep_rew_mean      | -105     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 2646     |\n",
      "|    total_timesteps  | 178595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.42     |\n",
      "|    n_updates        | 32148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-145.42 +/- 28.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -145     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 180000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.399    |\n",
      "|    n_updates        | 32499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 969      |\n",
      "|    ep_rew_mean      | -108     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 2753     |\n",
      "|    total_timesteps  | 182595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.309    |\n",
      "|    n_updates        | 33148    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-138.16 +/- 45.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -138     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 185000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.503    |\n",
      "|    n_updates        | 33749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 980      |\n",
      "|    ep_rew_mean      | -108     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 2863     |\n",
      "|    total_timesteps  | 186154   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.95     |\n",
      "|    n_updates        | 34038    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-145.93 +/- 6.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -146     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 190000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.495    |\n",
      "|    n_updates        | 34999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 980      |\n",
      "|    ep_rew_mean      | -108     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 63       |\n",
      "|    time_elapsed     | 2973     |\n",
      "|    total_timesteps  | 190154   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.535    |\n",
      "|    n_updates        | 35038    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 980      |\n",
      "|    ep_rew_mean      | -109     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 2978     |\n",
      "|    total_timesteps  | 194154   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.728    |\n",
      "|    n_updates        | 36038    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-163.98 +/- 12.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -164     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 195000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.379    |\n",
      "|    n_updates        | 36249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 980      |\n",
      "|    ep_rew_mean      | -108     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 3795     |\n",
      "|    total_timesteps  | 198154   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.312    |\n",
      "|    n_updates        | 37038    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-175.42 +/- 27.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -175     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.484    |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 980      |\n",
      "|    ep_rew_mean      | -107     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 3903     |\n",
      "|    total_timesteps  | 202154   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.416    |\n",
      "|    n_updates        | 38038    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-167.11 +/- 11.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -167     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 205000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.345    |\n",
      "|    n_updates        | 38749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 988      |\n",
      "|    ep_rew_mean      | -109     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 4010     |\n",
      "|    total_timesteps  | 206153   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.87     |\n",
      "|    n_updates        | 39038    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 991      |\n",
      "|    ep_rew_mean      | -107     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 4013     |\n",
      "|    total_timesteps  | 209699   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.398    |\n",
      "|    n_updates        | 39924    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-189.59 +/- 30.36\n",
      "Episode length: 981.00 +/- 38.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 981      |\n",
      "|    mean_reward      | -190     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 210000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.447    |\n",
      "|    n_updates        | 39999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 991      |\n",
      "|    ep_rew_mean      | -108     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 4122     |\n",
      "|    total_timesteps  | 213699   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.62     |\n",
      "|    n_updates        | 40924    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-158.13 +/- 25.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -158     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 215000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.693    |\n",
      "|    n_updates        | 41249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 991      |\n",
      "|    ep_rew_mean      | -111     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 4230     |\n",
      "|    total_timesteps  | 217699   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.42     |\n",
      "|    n_updates        | 41924    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-188.50 +/- 24.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -189     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 220000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.712    |\n",
      "|    n_updates        | 42499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 991      |\n",
      "|    ep_rew_mean      | -113     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 4338     |\n",
      "|    total_timesteps  | 221667   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.51     |\n",
      "|    n_updates        | 42916    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-49.70 +/- 133.28\n",
      "Episode length: 873.60 +/- 213.30\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 874      |\n",
      "|    mean_reward      | -49.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 225000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.59     |\n",
      "|    n_updates        | 43749    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 988      |\n",
      "|    ep_rew_mean      | -111     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 4433     |\n",
      "|    total_timesteps  | 225367   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.499    |\n",
      "|    n_updates        | 43841    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 985      |\n",
      "|    ep_rew_mean      | -112     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 4437     |\n",
      "|    total_timesteps  | 229091   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.695    |\n",
      "|    n_updates        | 44772    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-188.96 +/- 60.56\n",
      "Episode length: 988.60 +/- 22.80\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 989      |\n",
      "|    mean_reward      | -189     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 230000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.566    |\n",
      "|    n_updates        | 44999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 982      |\n",
      "|    ep_rew_mean      | -115     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 804      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 4545     |\n",
      "|    total_timesteps  | 232799   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.536    |\n",
      "|    n_updates        | 45699    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-34.20 +/- 152.30\n",
      "Episode length: 871.60 +/- 169.03\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 872      |\n",
      "|    mean_reward      | -34.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 235000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.373    |\n",
      "|    n_updates        | 46249    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 978      |\n",
      "|    ep_rew_mean      | -115     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 808      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 4642     |\n",
      "|    total_timesteps  | 236440   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.64     |\n",
      "|    n_updates        | 46609    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 970      |\n",
      "|    ep_rew_mean      | -114     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 812      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 4645     |\n",
      "|    total_timesteps  | 239584   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.638    |\n",
      "|    n_updates        | 47395    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-166.46 +/- 45.89\n",
      "Episode length: 914.00 +/- 106.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 914      |\n",
      "|    mean_reward      | -166     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 240000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.34     |\n",
      "|    n_updates        | 47499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 967      |\n",
      "|    ep_rew_mean      | -111     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 816      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 4747     |\n",
      "|    total_timesteps  | 243292   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.654    |\n",
      "|    n_updates        | 48322    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-161.42 +/- 73.19\n",
      "Episode length: 967.20 +/- 65.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 967      |\n",
      "|    mean_reward      | -161     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 245000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.359    |\n",
      "|    n_updates        | 48749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 962      |\n",
      "|    ep_rew_mean      | -111     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 820      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 4853     |\n",
      "|    total_timesteps  | 246812   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.335    |\n",
      "|    n_updates        | 49202    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-126.89 +/- 82.83\n",
      "Episode length: 890.00 +/- 135.26\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 890      |\n",
      "|    mean_reward      | -127     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 250000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.49     |\n",
      "|    n_updates        | 49999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 956      |\n",
      "|    ep_rew_mean      | -110     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 824      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 4951     |\n",
      "|    total_timesteps  | 250150   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.394    |\n",
      "|    n_updates        | 50037    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 953      |\n",
      "|    ep_rew_mean      | -113     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 828      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 4955     |\n",
      "|    total_timesteps  | 253937   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.349    |\n",
      "|    n_updates        | 50984    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-243.11 +/- 31.35\n",
      "Episode length: 881.60 +/- 116.64\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 882      |\n",
      "|    mean_reward      | -243     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 255000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.684    |\n",
      "|    n_updates        | 51249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 949      |\n",
      "|    ep_rew_mean      | -117     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 832      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 5052     |\n",
      "|    total_timesteps  | 257516   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.295    |\n",
      "|    n_updates        | 51878    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-245.70 +/- 82.91\n",
      "Episode length: 925.20 +/- 142.15\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 925      |\n",
      "|    mean_reward      | -246     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 260000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.326    |\n",
      "|    n_updates        | 52499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 948      |\n",
      "|    ep_rew_mean      | -118     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 836      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 5155     |\n",
      "|    total_timesteps  | 261425   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.679    |\n",
      "|    n_updates        | 52856    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=-222.59 +/- 38.76\n",
      "Episode length: 915.80 +/- 109.99\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 916      |\n",
      "|    mean_reward      | -223     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 265000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.449    |\n",
      "|    n_updates        | 53749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 945      |\n",
      "|    ep_rew_mean      | -118     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 840      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 5256     |\n",
      "|    total_timesteps  | 265099   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.22     |\n",
      "|    n_updates        | 53774    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 937      |\n",
      "|    ep_rew_mean      | -119     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 844      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 5260     |\n",
      "|    total_timesteps  | 268329   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.32     |\n",
      "|    n_updates        | 54582    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=270000, episode_reward=-113.13 +/- 176.68\n",
      "Episode length: 871.40 +/- 128.13\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 871      |\n",
      "|    mean_reward      | -113     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 270000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.506    |\n",
      "|    n_updates        | 54999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 934      |\n",
      "|    ep_rew_mean      | -114     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 848      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 5356     |\n",
      "|    total_timesteps  | 271989   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.663    |\n",
      "|    n_updates        | 55497    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-164.61 +/- 61.44\n",
      "Episode length: 926.20 +/- 76.38\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 926      |\n",
      "|    mean_reward      | -165     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 275000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.448    |\n",
      "|    n_updates        | 56249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 928      |\n",
      "|    ep_rew_mean      | -114     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 852      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 5457     |\n",
      "|    total_timesteps  | 275426   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.512    |\n",
      "|    n_updates        | 56356    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 928      |\n",
      "|    ep_rew_mean      | -113     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 856      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 5461     |\n",
      "|    total_timesteps  | 278977   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.347    |\n",
      "|    n_updates        | 57244    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-115.64 +/- 15.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -116     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 280000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.589    |\n",
      "|    n_updates        | 57499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 922      |\n",
      "|    ep_rew_mean      | -111     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 860      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 5571     |\n",
      "|    total_timesteps  | 282392   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.336    |\n",
      "|    n_updates        | 58097    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-84.00 +/- 67.90\n",
      "Episode length: 965.20 +/- 69.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 965      |\n",
      "|    mean_reward      | -84      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 285000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.347    |\n",
      "|    n_updates        | 58749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 914      |\n",
      "|    ep_rew_mean      | -109     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 864      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 5675     |\n",
      "|    total_timesteps  | 285556   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.422    |\n",
      "|    n_updates        | 58888    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 909      |\n",
      "|    ep_rew_mean      | -108     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 868      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 5681     |\n",
      "|    total_timesteps  | 289076   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.303    |\n",
      "|    n_updates        | 59768    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-15.61 +/- 161.37\n",
      "Episode length: 860.40 +/- 122.52\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 860      |\n",
      "|    mean_reward      | -15.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 290000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.324    |\n",
      "|    n_updates        | 59999    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 906      |\n",
      "|    ep_rew_mean      | -109     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 872      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 5775     |\n",
      "|    total_timesteps  | 292782   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.19     |\n",
      "|    n_updates        | 60695    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=-137.61 +/- 91.78\n",
      "Episode length: 993.20 +/- 13.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 993      |\n",
      "|    mean_reward      | -138     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 295000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.318    |\n",
      "|    n_updates        | 61249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 902      |\n",
      "|    ep_rew_mean      | -106     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 876      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 5882     |\n",
      "|    total_timesteps  | 296366   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.391    |\n",
      "|    n_updates        | 61591    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 901      |\n",
      "|    ep_rew_mean      | -110     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 880      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 5888     |\n",
      "|    total_timesteps  | 299803   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.415    |\n",
      "|    n_updates        | 62450    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-147.61 +/- 16.75\n",
      "Episode length: 981.00 +/- 38.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 981      |\n",
      "|    mean_reward      | -148     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.449    |\n",
      "|    n_updates        | 62499    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 891      |\n",
      "|    ep_rew_mean      | -106     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 884      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 5993     |\n",
      "|    total_timesteps  | 302816   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.281    |\n",
      "|    n_updates        | 63203    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=-173.15 +/- 59.44\n",
      "Episode length: 769.20 +/- 174.62\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 769      |\n",
      "|    mean_reward      | -173     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 305000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.327    |\n",
      "|    n_updates        | 63749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 886      |\n",
      "|    ep_rew_mean      | -99.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 888      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 6086     |\n",
      "|    total_timesteps  | 306340   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.276    |\n",
      "|    n_updates        | 64084    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 876      |\n",
      "|    ep_rew_mean      | -87.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 892      |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 6089     |\n",
      "|    total_timesteps  | 309300   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.802    |\n",
      "|    n_updates        | 64824    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "agent.learn(total_timesteps=1000000, \n",
    "            callback=eval_callback,\n",
    "            tb_log_name=\"Basic DQN Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then connect to the log using **TensorBoard** from the command line: \n",
    "\n",
    "`tensorboard --logdir ./log_tb_lunarlander/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then open TensorBoard in a browser, typically located at:\n",
    "\n",
    "`http://localhost:6006/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the EvalCallback outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_log = np.load('./logs_lunarlander/evaluations.npz')\n",
    "evaluation_log_df = pd.DataFrame({item: [np.mean(ep) for ep in evaluation_log[item]] for item in evaluation_log.files})\n",
    "ax = evaluation_log_df.loc[0:len(evaluation_log_df), 'results'].plot(color = 'lightgray', xlim = [-5, len(evaluation_log_df)], figsize = (10,5))\n",
    "evaluation_log_df['results'].rolling(5).mean().plot(color = 'black', xlim = [-5, len(evaluation_log_df)])\n",
    "ax.set_xticklabels(evaluation_log_df['timesteps'])\n",
    "ax.set_xlabel(\"Eval Episode\")\n",
    "plt.ylabel(\"Rolling Mean Cumulative Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load the best model saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent = sb3.dqn.DQN.load(\"./logs_lunarlander/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the agent in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(best_agent, \n",
    "                                                                eval_env, \n",
    "                                                                n_eval_episodes=10,\n",
    "                                                               render = True)\n",
    "print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "312.792px",
    "left": "1063px",
    "top": "86.9167px",
    "width": "159.365px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
