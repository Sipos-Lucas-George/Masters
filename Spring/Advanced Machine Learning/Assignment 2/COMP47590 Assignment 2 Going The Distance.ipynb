{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eQU7pJxQfod"
   },
   "source": [
    "# COMP47590 Advanced Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Student Name:** Lucas George Sipos\n",
    "- **Student Number:** 24292215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPgBaQLxQfoj"
   },
   "source": [
    "## Assignment 2: Going the Distance\n",
    "Uses the PPO actor-critic method to train a neural network to control a simple robot in the RacingCar environment from OpenAI gym (https://gym.openai.com/envs/RacingCar-v0/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZNiGMqsQfok"
   },
   "source": [
    "![Racing](racing_car.gif)\n",
    "\n",
    "The **action** space can be continuous or discreet. If **continuous** there are 3 actions :\n",
    "\n",
    "- 0: steering, -1 is full left, +1 is full right\n",
    "- 1: gas\n",
    "- 2: breaking\n",
    "\n",
    "If **discrete** there are 5 actions:\n",
    "- 0: do nothing\n",
    "- 1: steer left\n",
    "- 2: steer right\n",
    "- 3: gas\n",
    "- 4: brake\n",
    "\n",
    "For this assignment we should use the continuous action space. \n",
    "\n",
    "**Reward** of -0.1 is awarded every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points.\n",
    "\n",
    "And the default **observation** is a single image frame (96 * 96)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm_TwuoRQfol"
   },
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHWFxiANQfom"
   },
   "source": [
    "If using Google colab you need to install packages - comment out lines below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XEhFeXkQfom",
    "outputId": "42e5ffd2-1cd3-4daf-b535-82e7c00c2dde"
   },
   "source": [
    "#!apt install swig cmake ffmpeg\n",
    "#!apt-get install -y xvfb x11-utils\n",
    "# !pip install stable-baselines3[extra] pyglet box2d box2d-kengz\n",
    "# !pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV9IM5FRQfoo"
   },
   "source": [
    "For Google colab comment out this cell to make a virtual rendering canvas so render calls work (we still won't see display!)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zh2cw7iSQfop"
   },
   "source": [
    "#import pyvirtualdisplay\n",
    "#\n",
    "#_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "#                                    size=(1400, 900))\n",
    "#_ = _display.start()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8Bcx9GkQfoq"
   },
   "source": [
    "Import required packages. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dZcWCINuQfor",
    "ExecuteTime": {
     "end_time": "2025-04-16T18:09:51.974071Z",
     "start_time": "2025-04-16T18:09:50.771123Z"
    }
   },
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "import pandas as pd  # For data frames and data frame manipulation\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np  # For general  numeric operations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEJsAysfQfos"
   },
   "source": [
    "### Create and Explore the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCMqLB1VQfot"
   },
   "source": [
    "Create the **CarRacing-v2** environment. Add wrappers to resize the images and convert to greyscale."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7xkqRzcQfot",
    "outputId": "4a53b8ef-2971-4743-96b6-5172c8693737",
    "ExecuteTime": {
     "end_time": "2025-04-16T18:10:04.128143Z",
     "start_time": "2025-04-16T18:10:04.125515Z"
    }
   },
   "source": [
    "def make_env(render: str = \"rgb_array\"):\n",
    "    _env = gym.make('CarRacing-v2', render_mode=render)\n",
    "    _env = gym.wrappers.ResizeObservation(_env, (84,84))\n",
    "    _env = gym.wrappers.gray_scale_observation.GrayScaleObservation(_env, keep_dim=True)\n",
    "    # _env = gym.wrappers.TimeLimit(_env, max_episode_steps=2000)\n",
    "    _env = gym.wrappers.TimeLimit(_env, max_episode_steps=1500)\n",
    "    return _env"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T20:11:01.081553Z",
     "start_time": "2025-04-14T20:11:00.966706Z"
    }
   },
   "cell_type": "code",
   "source": "env = make_env()",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9BQTzRKQfou"
   },
   "source": [
    "Explore the environment - view the action space and observation space."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peo_ntRkQfou",
    "outputId": "e9170d3a-93e5-4305-89b8-6719ef0f4bde",
    "ExecuteTime": {
     "end_time": "2025-04-11T21:44:53.257065Z",
     "start_time": "2025-04-11T21:44:53.254395Z"
    }
   },
   "source": [
    "env.action_space"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.  0.  0.], 1.0, (3,), float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgObNy4uQfov",
    "outputId": "f319f83d-c61b-47e8-ccac-99e5b39fe4f5",
    "ExecuteTime": {
     "end_time": "2025-04-11T21:44:53.399901Z",
     "start_time": "2025-04-11T21:44:53.397670Z"
    }
   },
   "source": [
    "env.observation_space"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (84, 84, 1), uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-QsvRe0Qfow"
   },
   "source": [
    "Play an episode of the environment using random actions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_jvIhrjQfox",
    "outputId": "f12cbfa7-23a2-4efd-b58f-4729be5eb558",
    "ExecuteTime": {
     "end_time": "2025-04-11T21:45:03.584527Z",
     "start_time": "2025-04-11T21:44:59.710157Z"
    }
   },
   "source": [
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward: {total_reward}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -30.656934306569767\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ml3JnJ5Qfox"
   },
   "source": [
    "### Single Image Agent\n",
    "Create an agent that controls the car using a single image frame as the state input. We recommend a PPO agent with the following hyper-parameters (although you can experiment):\n",
    "- learning_rate = 3e-5\n",
    "- n_steps = 512\n",
    "- ent_coef = 0.001\n",
    "- batch_size = 128\n",
    "- gae_lambda =  0.9\n",
    "- n_epochs = 20\n",
    "- use_sde = True\n",
    "- sde_sample_freq = 4\n",
    "- clip_range = 0.4\n",
    "- policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "\n",
    "We also recommend enabling **tensorboard** monitoring of the training process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHY19WhjQfoy",
    "outputId": "38bcf412-18ea-40db-b6b5-b1bff5550f22",
    "ExecuteTime": {
     "end_time": "2025-04-14T20:11:04.271359Z",
     "start_time": "2025-04-14T20:11:03.763271Z"
    }
   },
   "source": [
    "agent = sb3.PPO(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    learning_rate=3e-5,\n",
    "    n_steps=512,\n",
    "    ent_coef=0.001,\n",
    "    batch_size=128,\n",
    "    gae_lambda=0.9,\n",
    "    n_epochs=20,\n",
    "    use_sde=True,\n",
    "    sde_sample_freq=4,\n",
    "    clip_range=0.4,\n",
    "    policy_kwargs={'log_std_init': -2, 'ortho_init': False},\n",
    "    tensorboard_log=\"./log_carracing_PPO/\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
      "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaVvGXkNQfoz"
   },
   "source": [
    "Examine the actor and critic network architectures."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNjosaimQfoz",
    "outputId": "c64af8eb-3cb9-4d18-f1cd-c3251ea3904a",
    "ExecuteTime": {
     "end_time": "2025-04-14T20:11:08.080195Z",
     "start_time": "2025-04-14T20:11:08.076855Z"
    }
   },
   "source": "agent.policy",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticCnnPolicy(\n",
       "  (features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pi_features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (vf_features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential()\n",
       "    (value_net): Sequential()\n",
       "  )\n",
       "  (action_net): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (value_net): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "pdRpUwTeQfoz"
   },
   "cell_type": "markdown",
   "source": "Create an evaluation callback that is called every at regular intervals and renders the episode."
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oulh4r78Qfoz",
    "outputId": "876d8029-1ce4-46b7-e338-6643dadb8dd9",
    "ExecuteTime": {
     "end_time": "2025-04-14T20:11:14.870057Z",
     "start_time": "2025-04-14T20:11:14.865906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_env = make_env()\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    eval_freq=10_000,\n",
    "    render=True,\n",
    "    best_model_save_path=\"./best_model/\",\n",
    "    log_path=\"./log_carracing_eval/\",\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
      "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMj3hpimQfo0"
   },
   "source": [
    "Train the model for a large number of timesteps (500,000 timesteps will probably work well)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNtGLDLUQfo0",
    "outputId": "1bc23c12-4702-4676-88b7-c5e7b632c887"
   },
   "source": [
    "agent.learn(\n",
    "    total_timesteps=500_000,\n",
    "    callback=eval_callback\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24RlrYF7Qfo0"
   },
   "source": [
    "Connect to the tensorboard log using **TensorBoard** from the command line to view training progress: \n",
    "\n",
    "`tensorboard --logdir ./logs_carracing_PPO/`\n",
    "\n",
    "Then open TensorBoard in a browser, typically located at:\n",
    "\n",
    "`http://localhost:6006/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nir4ZmnqQfo1"
   },
   "source": [
    "Save the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CBGtMv04Qfo1",
    "ExecuteTime": {
     "end_time": "2025-04-14T21:08:35.676881Z",
     "start_time": "2025-04-11T23:53:58.685742Z"
    }
   },
   "source": "agent.save(\"ppo_carracing_trained\")",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxs3h7Owre5T"
   },
   "source": [
    "For memory management delete old agent and environment (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uOGiJDrTrdV3",
    "ExecuteTime": {
     "end_time": "2025-04-14T21:08:51.275005Z",
     "start_time": "2025-04-14T21:08:51.272956Z"
    }
   },
   "source": [
    "del agent\n",
    "del env\n",
    "del eval_env"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQF_FjMEQfo1"
   },
   "source": [
    "### Create Image Stack Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the CarRacing-v0 environment using wrappers to resize the images to 64 x 64 and change to greyscale. Also add a wrapper to create a stack of 4 frames. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ijzQwZY0Qfo2",
    "ExecuteTime": {
     "end_time": "2025-04-16T18:10:25.985794Z",
     "start_time": "2025-04-16T18:10:25.982822Z"
    }
   },
   "source": [
    "def make_env_tb(render: str = \"rgb_array\"):\n",
    "    _env = gym.make('CarRacing-v2', render_mode=render)\n",
    "    _env = gym.wrappers.resize_observation.ResizeObservation(_env, (84,84))\n",
    "    _env = gym.wrappers.gray_scale_observation.GrayScaleObservation(_env, keep_dim=True)\n",
    "    # _env = gym.wrappers.TimeLimit(_env, max_episode_steps=2000)\n",
    "    _env = gym.wrappers.TimeLimit(_env, max_episode_steps=1500)\n",
    "\n",
    "    _env = sb3.common.monitor.Monitor(_env)\n",
    "    _env = sb3.common.vec_env.DummyVecEnv([lambda: _env])\n",
    "    _env = sb3.common.vec_env.VecFrameStack(_env, n_stack=4)\n",
    "    return _env"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:08:53.794984Z",
     "start_time": "2025-04-14T21:08:53.791520Z"
    }
   },
   "cell_type": "code",
   "source": "env = make_env_tb()",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77o0YzOmQfo2"
   },
   "source": [
    "Create an agent that controls the car using a stack of input image frames as the state input. We recommend a PPO agent with the following hyper-parameters (although you can experiment):\n",
    "- learning_rate = 3e-5\n",
    "- n_steps = 512\n",
    "- ent_coef = 0.001\n",
    "- batch_size = 128\n",
    "- gae_lambda =  0.9\n",
    "- n_epochs = 20\n",
    "- use_sde = True\n",
    "- sde_sample_freq = 4\n",
    "- clip_range = 0.4\n",
    "- policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "\n",
    "We also recommend enabling **tensorboard** monitoring of the training process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4aAxmNANQfo2",
    "ExecuteTime": {
     "end_time": "2025-04-14T21:08:55.776406Z",
     "start_time": "2025-04-14T21:08:55.765287Z"
    }
   },
   "source": [
    "agent = sb3.PPO(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    learning_rate=3e-5,\n",
    "    n_steps=512,\n",
    "    ent_coef=0.001,\n",
    "    batch_size=128,\n",
    "    gae_lambda=0.9,\n",
    "    n_epochs=20,\n",
    "    use_sde=True,\n",
    "    sde_sample_freq=4,\n",
    "    clip_range=0.4,\n",
    "    policy_kwargs={'log_std_init': -2, 'ortho_init': False},\n",
    "    tensorboard_log=\"./log_tb_carracing_PPO/\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkOXkC-HQfo2"
   },
   "source": [
    "Examine the actor and critic network architectures."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fS-fpmWwQfo3",
    "ExecuteTime": {
     "end_time": "2025-04-14T21:09:00.479498Z",
     "start_time": "2025-04-14T21:09:00.476568Z"
    }
   },
   "source": "agent.policy",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticCnnPolicy(\n",
       "  (features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pi_features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (vf_features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential()\n",
       "    (value_net): Sequential()\n",
       "  )\n",
       "  (action_net): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (value_net): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1jObjvdQfo3"
   },
   "source": [
    "Create an evaluation callback that is called every at regular intervals and renders the episode."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aL2c4LiOQfo4",
    "ExecuteTime": {
     "end_time": "2025-04-14T21:09:02.129655Z",
     "start_time": "2025-04-14T21:09:02.125472Z"
    }
   },
   "source": [
    "eval_env = make_env_tb()\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    eval_freq=10_000,\n",
    "    render=True,\n",
    "    best_model_save_path=\"./best_model_tb/\",\n",
    "    log_path=\"./log_tb_carracing_eval/\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s33dR-mgQfo4"
   },
   "source": [
    "Train the model for a large number of timesteps (500,000 timesteps will probably work well)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lRj1j3xjQfo4"
   },
   "source": [
    "agent.learn(\n",
    "    total_timesteps=500_000,\n",
    "    callback=eval_callback\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sotpjktQfo4"
   },
   "source": [
    "Connect to the tensorboard log using **TensorBoard** from the command line to view training progress: \n",
    "\n",
    "`tensorboard --logdir ./log_tb_carracing_PPO/`\n",
    "\n",
    "Then open TensorBoard in a browser, typically located at:\n",
    "\n",
    "`http://localhost:6006/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8j9GrTTQfo5"
   },
   "source": [
    "Save the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z8ivh_VpQfo5",
    "ExecuteTime": {
     "end_time": "2025-04-14T21:08:35.708393Z",
     "start_time": "2025-04-11T21:34:08.214216Z"
    }
   },
   "source": "agent.save(\"ppo_tb_carracing_trained\")",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory management delete old agent and environment (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "del agent\n",
    "del env\n",
    "del eval_env"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWZrzrZ3Qfo5"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRzc_-yOQfo5"
   },
   "source": [
    "Load the single image saved agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XyfQmsF6Qfo6",
    "ExecuteTime": {
     "end_time": "2025-04-16T18:13:31.408256Z",
     "start_time": "2025-04-16T18:13:31.371091Z"
    }
   },
   "source": [
    "agent = sb3.PPO.load(\"v6/ppo_carracing_trained.zip\")\n",
    "agent_best = sb3.PPO.load(\"v6/best_model/best_model.zip\")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the single image environment for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T18:13:32.929107Z",
     "start_time": "2025-04-16T18:13:32.925782Z"
    }
   },
   "source": [
    "eval_env = make_env()\n",
    "n_episodes = 30"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8X8r5S5Qfo6"
   },
   "source": "Evaluate the agent in the environment for 30 episodes, rendering the process."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T18:18:24.002470Z",
     "start_time": "2025-04-16T18:14:38.285264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent,\n",
    "                                                                eval_env,\n",
    "                                                                n_eval_episodes=n_episodes,\n",
    "                                                                render=True)\n",
    "print(\"Trained Agent Mean Reward: {} +/- {}\".format(mean_reward, std_reward))\n",
    "\n",
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent_best,\n",
    "                                                                eval_env,\n",
    "                                                                n_eval_episodes=n_episodes,\n",
    "                                                                render=True)\n",
    "print(\"Best Agent Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
      "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n",
      "/opt/anaconda3/envs/AI/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/AI/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:259: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Agent Mean Reward: -93.17763194491467 +/- 0.5450886293190688\n",
      "Best Agent Mean Reward: 99.82978916689754 +/- 125.32855192728414\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6K6TXdCQfo6"
   },
   "source": [
    "For memory management delete the single image agent (assumes variable names - change if required)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yAeeHAKVQfo6",
    "ExecuteTime": {
     "end_time": "2025-04-16T18:18:24.028004Z",
     "start_time": "2025-04-16T18:18:24.023445Z"
    }
   },
   "source": [
    "del agent\n",
    "del agent_best\n",
    "del eval_env"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1y0UngLQfo6"
   },
   "source": [
    "Load the image stack agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lCxi6SvMQfo7",
    "ExecuteTime": {
     "end_time": "2025-04-16T18:18:24.080976Z",
     "start_time": "2025-04-16T18:18:24.044403Z"
    }
   },
   "source": [
    "agent = sb3.PPO.load(\"v6/ppo_tb_carracing_trained.zip\")\n",
    "agent_best = sb3.PPO.load(\"v6/best_model_tb/best_model.zip\")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the image stack environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T18:18:24.089909Z",
     "start_time": "2025-04-16T18:18:24.085654Z"
    }
   },
   "source": [
    "eval_env = make_env_tb()\n",
    "n_episodes = 30"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr8lGKjpQfo7"
   },
   "source": "Evaluate the agent in the environment for 30 episodes, rendering the process."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T18:22:55.080972Z",
     "start_time": "2025-04-16T18:18:37.812738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent,\n",
    "                                                                eval_env,\n",
    "                                                                n_eval_episodes=n_episodes,\n",
    "                                                                render=True)\n",
    "print(\"Trained Agent Mean Reward: {} +/- {}\".format(mean_reward, std_reward))\n",
    "\n",
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent_best,\n",
    "                                                                eval_env,\n",
    "                                                                n_eval_episodes=n_episodes,\n",
    "                                                                render=True)\n",
    "print(\"Best Agent Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Agent Mean Reward: -86.01198786666667 +/- 14.52192114452869\n",
      "Best Agent Mean Reward: 378.30467026666673 +/- 138.96342264659938\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRCxNiPqQfo7"
   },
   "source": [
    "Reflect on which  agent performs better at the task, and the training process involved (max 200 words)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this task, we compared two agents trained on the `CarRacing-v2` environment: one using single grayscale frames, and the other using a stack of four consecutive grayscale frames. Although both agents struggled during training (having suboptimal behaviors like doing donuts) the best version of the stacked image agent performed significantly better in evaluation. Specifically, it achieved a mean reward of `378.30 +/- 138.96`, compared to the single image agent’s `99.82 +/- 125.33`.\n",
    "\n",
    "The training environments were nearly identical, with the primary difference being the use of VecFrameStack for the stacked agent. This image stacking allowed the agent to infer motion and momentum, providing better context for decision-making in a dynamic environment. In contrast, the single image agent lacked this temporal awareness, which likely made it worse when learning effective driving strategies.\n",
    "\n",
    "Despite the training being mostly ineffective, the final evaluation suggests that the stacked image agent is far superior for this task. The stark difference in reward highlights the importance of temporal information in environments like CarRacing, where understanding speed, direction, and continuity between frames is a big deal.\n",
    "\n",
    "Overall, the stacked image approach proves to be a more effective representation for training agents in continuous control tasks."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "WS3 Going The Distance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.6875px",
    "left": "1058px",
    "top": "147.125px",
    "width": "159.359px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
